{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "714eed1f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/media/ssd/test/gnn_cuda_env/lib/python3.8/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /media/ssd/test/gnn_cuda_env/lib/python3.8/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/media/ssd/test/gnn_cuda_env/lib/python3.8/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /media/ssd/test/gnn_cuda_env/lib/python3.8/site-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
            "/media/ssd/test/gnn_cuda_env/lib/python3.8/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /media/ssd/test/gnn_cuda_env/lib/python3.8/site-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
            "  warnings.warn(\n",
            "/media/ssd/test/gnn_cuda_env/lib/python3.8/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /media/ssd/test/gnn_cuda_env/lib/python3.8/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
            "2025-04-13 20:58:01,500 - INFO - Loading dataset from: /media/ssd/test/standardized-datasets/combined/combined_unsw_cicRed_botRed_netflow_10pct.csv\n",
            "2025-04-13 20:58:05,102 - INFO - Dataset loaded successfully. Shape: (655094, 47)\n",
            "2025-04-13 20:58:05,103 - INFO - All required columns are present.\n",
            "2025-04-13 20:58:05,327 - INFO - Memory usage: 382.78 MB\n",
            "2025-04-13 20:58:05,329 - INFO - Starting graph construction...\n",
            "2025-04-13 20:58:06,014 - INFO - Created source and destination IP:Port identifiers.\n",
            "2025-04-13 20:58:06,304 - INFO - Found 611553 unique IP:Port nodes.\n",
            "2025-04-13 20:58:06,470 - INFO - Created mapping for 611553 nodes.\n",
            "/tmp/ipykernel_142197/327083079.py:81: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  edge_index = torch.tensor([src_ids, dst_ids], dtype=torch.long)\n",
            "2025-04-13 20:58:07,430 - INFO - Created edge_index tensor with shape: torch.Size([2, 655094])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of flows (edges): 655094\n",
            "Number of unique IP:Port nodes: 611553\n",
            "Edge index shape: torch.Size([2, 655094])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- Configuration ---\n",
        "DATASET_PATH = \"/media/ssd/test/standardized-datasets/combined/combined_unsw_cicRed_botRed_netflow_10pct.csv\"\n",
        "# Define required columns based on design doc and typical NetFlow features\n",
        "REQUIRED_COLUMNS = [\n",
        "    'IPV4_SRC_ADDR', 'L4_SRC_PORT', 'IPV4_DST_ADDR', 'L4_DST_PORT',\n",
        "    'PROTOCOL', 'L7_PROTO', 'IN_BYTES', 'OUT_BYTES', 'IN_PKTS', 'OUT_PKTS',\n",
        "    'TCP_FLAGS', 'FLOW_DURATION_MILLISECONDS', 'Label', 'Attack', 'dataset_source', 'flow_id'\n",
        "]\n",
        "\n",
        "# --- 1. Data Loading ---\n",
        "logging.info(f\"Loading dataset from: {DATASET_PATH}\")\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    logging.error(f\"Dataset file not found at {DATASET_PATH}\")\n",
        "    # Handle error appropriately, maybe raise FileNotFoundError\n",
        "    raise FileNotFoundError(f\"Dataset file not found at {DATASET_PATH}\")\n",
        "\n",
        "try:\n",
        "    # Load only necessary columns to save memory initially if needed, or load all\n",
        "    # Consider using low_memory=False if dtype warnings appear\n",
        "    df = pd.read_csv(DATASET_PATH, low_memory=False)\n",
        "    logging.info(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
        "\n",
        "    # Verify required columns\n",
        "    missing_cols = [col for col in REQUIRED_COLUMNS if col not in df.columns]\n",
        "    if missing_cols:\n",
        "        logging.error(f\"Missing required columns: {missing_cols}\")\n",
        "        raise ValueError(f\"Dataset missing required columns: {missing_cols}\")\n",
        "    logging.info(\"All required columns are present.\")\n",
        "\n",
        "    # --- Optional: Basic Cleaning/Info ---\n",
        "    logging.info(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
        "    # print(df.info())\n",
        "    # print(df.head())\n",
        "    # Handle potential NaNs if necessary (standardization should minimize this)\n",
        "    # df.dropna(subset=['IPV4_SRC_ADDR', 'L4_SRC_PORT', 'IPV4_DST_ADDR', 'L4_DST_PORT'], inplace=True)\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error during data loading or initial verification: {e}\")\n",
        "    raise\n",
        "\n",
        "# --- 2. Graph Construction ---\n",
        "logging.info(\"Starting graph construction...\")\n",
        "\n",
        "# Convert ports to string and handle potential float representations if necessary\n",
        "df['L4_SRC_PORT'] = df['L4_SRC_PORT'].astype(int).astype(str)\n",
        "df['L4_DST_PORT'] = df['L4_DST_PORT'].astype(int).astype(str)\n",
        "\n",
        "# Combine IP and Port to create unique node identifiers\n",
        "df['src_ip_port'] = df['IPV4_SRC_ADDR'] + ':' + df['L4_SRC_PORT']\n",
        "df['dst_ip_port'] = df['IPV4_DST_ADDR'] + ':' + df['L4_DST_PORT']\n",
        "logging.info(\"Created source and destination IP:Port identifiers.\")\n",
        "\n",
        "# Get all unique IP:Port combinations involved in flows\n",
        "unique_src_nodes = df['src_ip_port'].unique()\n",
        "unique_dst_nodes = df['dst_ip_port'].unique()\n",
        "all_unique_nodes = pd.unique(np.concatenate((unique_src_nodes, unique_dst_nodes)))\n",
        "logging.info(f\"Found {len(all_unique_nodes)} unique IP:Port nodes.\")\n",
        "\n",
        "# Create mapping from IP:Port string to integer node ID\n",
        "ip_port_to_id = {ip_port: i for i, ip_port in enumerate(all_unique_nodes)}\n",
        "num_nodes = len(all_unique_nodes)\n",
        "logging.info(f\"Created mapping for {num_nodes} nodes.\")\n",
        "\n",
        "# Map source and destination IP:Ports to their integer IDs\n",
        "src_ids = df['src_ip_port'].map(ip_port_to_id).values\n",
        "dst_ids = df['dst_ip_port'].map(ip_port_to_id).values\n",
        "\n",
        "# Create edge_index tensor [2, num_edges]\n",
        "# Edges represent flows from source node to destination node\n",
        "edge_index = torch.tensor([src_ids, dst_ids], dtype=torch.long)\n",
        "logging.info(f\"Created edge_index tensor with shape: {edge_index.shape}\")\n",
        "\n",
        "# --- Display some results ---\n",
        "print(f\"Number of flows (edges): {len(df)}\")\n",
        "print(f\"Number of unique IP:Port nodes: {num_nodes}\")\n",
        "print(f\"Edge index shape: {edge_index.shape}\")\n",
        "# print(\"First 5 rows with node IDs:\")\n",
        "# print(df[['src_ip_port', 'dst_ip_port']].head())\n",
        "# print(f\"Source IDs (first 5): {src_ids[:5]}\")\n",
        "# print(f\"Destination IDs (first 5): {dst_ids[:5]}\")\n",
        "# print(f\"Edge Index (first 5 columns):\\n{edge_index[:, :5]}\")\n",
        "\n",
        "# Keep relevant data for next steps\n",
        "flows_df = df # Keep the dataframe for feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "582929fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-13 20:58:10,797 - INFO - Starting imbalanced sampling. Target size for large classes: 50000\n",
            "2025-04-13 20:58:10,804 - INFO - Original class distribution:\\nLabel\n",
            "0    341806\n",
            "1    313288\n",
            "Name: count, dtype: int64\n",
            "2025-04-13 20:58:10,805 - INFO - Found 2 large classes (>= 1000 samples).\n",
            "2025-04-13 20:58:10,806 - INFO - Scaling factor for large classes: 0.0763\n",
            "2025-04-13 20:58:10,806 - INFO -   Sampling class '0': target size=26088, final sample size=26088\n",
            "2025-04-13 20:58:10,938 - INFO -   Sampling class '1': target size=23911, final sample size=23911\n",
            "2025-04-13 20:58:11,097 - INFO - Finished sampling. New dataset size: 49999\n",
            "2025-04-13 20:58:11,099 - INFO - New class distribution:\\nLabel\n",
            "0    26088\n",
            "1    23911\n",
            "Name: count, dtype: int64\n",
            "2025-04-13 20:58:11,102 - INFO - Sampling successful. Sampled DataFrame shape: (49999, 49)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled DataFrame shape: (49999, 49)\n",
            "Sampled DataFrame memory usage: 41.30 MB\n"
          ]
        }
      ],
      "source": [
        "# Cell 1.5: Data Sampling (New Cell)\n",
        "import numpy as np\n",
        "import logging\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# --- Sampling Configuration ---\n",
        "# Adjust these values based on memory capacity and desired dataset size\n",
        "SAMPLED_SIZE_LARGE_CLASSES = 50000  # Target size for *sum* of samples from large classes\n",
        "MIN_LARGE_CLASS_SIZE = 1000 # Threshold to consider a class 'large'\n",
        "\n",
        "# --- Sampling Function ---\n",
        "def create_imbalanced_subset(df, target_col, new_dataset_size_large_classes, min_large_class_size):\n",
        "    \"\"\"\n",
        "    Create a smaller dataset while preserving class imbalance, focusing on reducing majority classes.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Original dataset.\n",
        "        target_col (str): Name of the target label column.\n",
        "        new_dataset_size_large_classes (int): Target total number of samples from classes exceeding min_large_class_size.\n",
        "        min_large_class_size (int): Minimum number of samples for a class to be considered 'large'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A reduced dataset.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Starting imbalanced sampling. Target size for large classes: {new_dataset_size_large_classes}\")\n",
        "    class_counts = df[target_col].value_counts()\n",
        "    logging.info(f\"Original class distribution:\\\\n{class_counts}\")\n",
        "\n",
        "    large_classes = class_counts[class_counts >= min_large_class_size]\n",
        "    small_classes = class_counts[class_counts < min_large_class_size]\n",
        "\n",
        "    total_large_samples_original = large_classes.sum()\n",
        "    num_large_classes = len(large_classes)\n",
        "\n",
        "    sampled_data = []\n",
        "\n",
        "    if num_large_classes > 0 and total_large_samples_original > 0:\n",
        "        logging.info(f\"Found {num_large_classes} large classes (>= {min_large_class_size} samples).\")\n",
        "        # Calculate scaling factor based on the sum of large classes\n",
        "        scaling_factor = min(1.0, new_dataset_size_large_classes / total_large_samples_original)\n",
        "        logging.info(f\"Scaling factor for large classes: {scaling_factor:.4f}\")\n",
        "\n",
        "        # Sample from large classes proportionally\n",
        "        for class_label, original_count in large_classes.items():\n",
        "            # Calculate proportional target size\n",
        "            target_size = int(original_count * scaling_factor)\n",
        "            # Ensure we don't sample more than available and respect min_large_class_size if scaled size is too small\n",
        "            sample_size = max(1, min(target_size, original_count)) # Ensure at least 1 sample, don't exceed original count\n",
        "            logging.info(f\"  Sampling class '{class_label}': target size={target_size}, final sample size={sample_size}\")\n",
        "            sampled_class_df = df[df[target_col] == class_label].sample(n=sample_size, random_state=42, replace=False)\n",
        "            sampled_data.append(sampled_class_df)\n",
        "    else:\n",
        "        logging.info(\"No large classes found or large classes sum to zero.\")\n",
        "\n",
        "    # Keep all samples from small classes\n",
        "    if not small_classes.empty:\n",
        "        logging.info(f\"Keeping all samples for {len(small_classes)} small classes (< {min_large_class_size} samples).\")\n",
        "        small_class_df = df[df[target_col].isin(small_classes.index)]\n",
        "        sampled_data.append(small_class_df)\n",
        "\n",
        "    if not sampled_data:\n",
        "        logging.warning(\"Sampling resulted in an empty dataset.\")\n",
        "        return pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    # Concatenate sampled dataframes\n",
        "    df_sampled = pd.concat(sampled_data).reset_index(drop=True)\n",
        "    logging.info(f\"Finished sampling. New dataset size: {len(df_sampled)}\")\n",
        "    logging.info(f\"New class distribution:\\\\n{df_sampled[target_col].value_counts()}\")\n",
        "\n",
        "    return df_sampled\n",
        "\n",
        "# --- Apply Sampling ---\n",
        "# Assuming 'df' is loaded from Cell 1 and 'Label' is the target column\n",
        "try:\n",
        "    # Ensure 'Label' column exists\n",
        "    if 'Label' not in df.columns:\n",
        "        raise KeyError(\"Target column 'Label' not found in DataFrame 'df'. Verify column names.\")\n",
        "\n",
        "    df_sampled = create_imbalanced_subset(\n",
        "        df,\n",
        "        target_col='Label',\n",
        "        new_dataset_size_large_classes=SAMPLED_SIZE_LARGE_CLASSES,\n",
        "        min_large_class_size=MIN_LARGE_CLASS_SIZE\n",
        "    )\n",
        "    # Clean up original large dataframe if memory is tight\n",
        "    # del df\n",
        "    # gc.collect()\n",
        "except NameError:\n",
        "    logging.error(\"Original DataFrame 'df' not found. Ensure Cell 1 executed successfully.\")\n",
        "    df_sampled = None # Set to None to indicate failure\n",
        "except KeyError as e:\n",
        "    logging.error(e)\n",
        "    df_sampled = None # Set to None to indicate failure\n",
        "except Exception as e:\n",
        "     logging.error(f\"An unexpected error occurred during sampling: {e}\")\n",
        "     df_sampled = None\n",
        "\n",
        "# --- Check if sampling was successful ---\n",
        "if df_sampled is not None and not df_sampled.empty:\n",
        "    logging.info(f\"Sampling successful. Sampled DataFrame shape: {df_sampled.shape}\")\n",
        "    print(f\"Sampled DataFrame shape: {df_sampled.shape}\")\n",
        "    print(f\"Sampled DataFrame memory usage: {df_sampled.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
        "else:\n",
        "    logging.error(\"Sampling failed or resulted in an empty DataFrame. Stopping execution.\")\n",
        "    # Optionally raise an error to stop the notebook\n",
        "    raise ValueError(\"Sampling failed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0437f762",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-13 20:58:17,614 - INFO - Starting feature engineering on sampled data (Shape: (49999, 49))...\n",
            "2025-04-13 20:58:17,618 - INFO - DataFrame shape after selecting processing columns: (49999, 9)\n",
            "2025-04-13 20:58:17,618 - INFO - Processing numerical features: ['IN_BYTES', 'OUT_BYTES', 'IN_PKTS', 'OUT_PKTS', 'FLOW_DURATION_MILLISECONDS']\n",
            "2025-04-13 20:58:17,629 - INFO - Applied Log1p and StandardScaler to numerical features.\n",
            "2025-04-13 20:58:17,630 - INFO - Processing categorical features: ['PROTOCOL', 'L7_PROTO', 'TCP_FLAGS']\n",
            "2025-04-13 20:58:17,698 - INFO - Unique values count in 'PROTOCOL': 220\n",
            "2025-04-13 20:58:17,701 - INFO - Unique values count in 'L7_PROTO': 112\n",
            "2025-04-13 20:58:17,704 - INFO - Unique values count in 'TCP_FLAGS': 28\n",
            "2025-04-13 20:58:17,704 - INFO - Applying One-Hot Encoding (pd.get_dummies) to categorical features...\n",
            "2025-04-13 20:58:17,856 - INFO - One-Hot Encoding resulted in 360 new feature columns.\n",
            "2025-04-13 20:58:17,857 - INFO - Shape of scaled numerical: (49999, 5)\n",
            "2025-04-13 20:58:17,857 - INFO - Shape of encoded categorical: (49999, 360)\n",
            "2025-04-13 20:58:18,127 - INFO - Combined features into matrix X with shape: (49999, 365)\n",
            "2025-04-13 20:58:18,129 - INFO - Created labels vector y with shape: (49999,)\n",
            "2025-04-13 20:58:18,130 - INFO - Label distribution in y: {0: 26088, 1: 23911}\n",
            "2025-04-13 20:58:18,284 - INFO - Feature engineering on sampled data complete. Produced feature matrix X and label vector y.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Feature Matrix X shape: (49999, 365)\n",
            "Final Label Vector y shape: (49999,)\n",
            "Saved 365 feature names to /media/ssd/test/GNN/Standardized Models/CAGN-GAT/cagn_gat_feature_list.pkl\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Feature Engineering (Revised for Tabular Output X, y on Sampled Data)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import logging\n",
        "import gc\n",
        "\n",
        "# --- Ensure df_sampled is available from the previous cell ---\n",
        "if 'df_sampled' not in locals() or df_sampled is None or df_sampled.empty:\n",
        "    logging.error(\"Sampled DataFrame 'df_sampled' not available or empty. Cannot proceed with feature engineering.\")\n",
        "    # Optionally raise an error\n",
        "    raise NameError(\"Sampled DataFrame 'df_sampled' is required but not available.\")\n",
        "\n",
        "# --- 3. Feature Engineering on Sampled Data ---\n",
        "logging.info(f\"Starting feature engineering on sampled data (Shape: {df_sampled.shape})...\")\n",
        "\n",
        "# Define feature columns based on design doc\n",
        "numerical_cols = [\n",
        "    'IN_BYTES', 'OUT_BYTES', 'IN_PKTS', 'OUT_PKTS',\n",
        "    'FLOW_DURATION_MILLISECONDS'\n",
        "]\n",
        "categorical_cols = [\n",
        "    'PROTOCOL', 'L7_PROTO', 'TCP_FLAGS'\n",
        "]\n",
        "label_col = 'Label'\n",
        "\n",
        "# Select relevant columns for processing X and y from the sampled data\n",
        "columns_to_process = numerical_cols + categorical_cols + [label_col]\n",
        "\n",
        "# Ensure all needed columns exist in df_sampled\n",
        "missing_processing_cols = [col for col in columns_to_process if col not in df_sampled.columns]\n",
        "if missing_processing_cols:\n",
        "     logging.error(f\"Sampled DataFrame missing required columns for processing: {missing_processing_cols}\")\n",
        "     raise ValueError(f\"Missing columns in df_sampled: {missing_processing_cols}\")\n",
        "\n",
        "# Create a working copy for processing\n",
        "flows_df_processed = df_sampled[columns_to_process].copy()\n",
        "logging.info(f\"DataFrame shape after selecting processing columns: {flows_df_processed.shape}\")\n",
        "\n",
        "\n",
        "# --- 3.1 Process Numerical Features ---\n",
        "logging.info(f\"Processing numerical features: {numerical_cols}\")\n",
        "# Ensure numeric types and handle potential errors/NaNs\n",
        "for col in numerical_cols:\n",
        "    flows_df_processed[col] = pd.to_numeric(flows_df_processed[col], errors='coerce')\n",
        "# Fill NaNs created by coercion or existing NaNs (e.g., with 0 or median)\n",
        "flows_df_processed[numerical_cols] = flows_df_processed[numerical_cols].fillna(0)\n",
        "\n",
        "# Log transform (log1p to handle zeros) - often good for byte/packet counts\n",
        "# Important: Apply log1p *before* scaling\n",
        "log_transformed_features = np.log1p(flows_df_processed[numerical_cols].values)\n",
        "\n",
        "# Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "# Fit scaler only on numerical features, transform the log-transformed values\n",
        "scaled_numerical_features = scaler.fit_transform(log_transformed_features)\n",
        "logging.info(\"Applied Log1p and StandardScaler to numerical features.\")\n",
        "\n",
        "# --- 3.2 Process Categorical Features ---\n",
        "logging.info(f\"Processing categorical features: {categorical_cols}\")\n",
        "# Ensure categorical columns are treated as such (e.g., strings or category type)\n",
        "# Convert to string first to handle potential mixed types or NaNs consistently\n",
        "flows_df_processed[categorical_cols] = flows_df_processed[categorical_cols].astype(str)\n",
        "# Fill any remaining NaNs in categorical columns (though astype(str) should handle most)\n",
        "flows_df_processed[categorical_cols] = flows_df_processed[categorical_cols].fillna('missing') # Or another placeholder\n",
        "\n",
        "\n",
        "# Check cardinality (optional but good practice, especially after sampling)\n",
        "for col in categorical_cols:\n",
        "    unique_count = flows_df_processed[col].nunique()\n",
        "    logging.info(f\"Unique values count in '{col}': {unique_count}\")\n",
        "    if unique_count > 1000: # Example threshold\n",
        "         logging.warning(f\"High cardinality in '{col}' ({unique_count}). OneHotEncoding might lead to very high dimensions.\")\n",
        "\n",
        "\n",
        "# One-Hot Encode categorical features using get_dummies\n",
        "logging.info(\"Applying One-Hot Encoding (pd.get_dummies) to categorical features...\")\n",
        "# Using pandas get_dummies is often simpler for DataFrames\n",
        "# Ensure consistent prefixing and handle potential NaN columns if 'missing' wasn't used for fillna\n",
        "categorical_encoded_df = pd.get_dummies(\n",
        "    flows_df_processed[categorical_cols],\n",
        "    columns=categorical_cols,\n",
        "    prefix=categorical_cols,\n",
        "    dummy_na=False, # Set to True if NaNs weren't filled and you want explicit NaN columns\n",
        "    dtype=int # Ensure resulting columns are integer type\n",
        ")\n",
        "logging.info(f\"One-Hot Encoding resulted in {categorical_encoded_df.shape[1]} new feature columns.\")\n",
        "\n",
        "\n",
        "# --- 3.3 Combine Features into Matrix X ---\n",
        "# Concatenate scaled numerical features and one-hot encoded categorical features\n",
        "# Ensure alignment by using the index from flows_df_processed\n",
        "logging.info(f\"Shape of scaled numerical: {scaled_numerical_features.shape}\")\n",
        "logging.info(f\"Shape of encoded categorical: {categorical_encoded_df.shape}\")\n",
        "\n",
        "# Convert numerical features to DataFrame to align indices easily\n",
        "scaled_numerical_df = pd.DataFrame(scaled_numerical_features, index=flows_df_processed.index, columns=numerical_cols)\n",
        "\n",
        "# Combine the feature DataFrames\n",
        "# Make sure indices match before concatenation\n",
        "X_features_list = [scaled_numerical_df, categorical_encoded_df.set_index(scaled_numerical_df.index)] # Align indices\n",
        "X_df = pd.concat(X_features_list, axis=1)\n",
        "\n",
        "# Convert the final feature DataFrame to a NumPy array\n",
        "X = X_df.values\n",
        "logging.info(f\"Combined features into matrix X with shape: {X.shape}\")\n",
        "\n",
        "# --- 3.4 Prepare Labels Vector y ---\n",
        "y = flows_df_processed[label_col].values.astype(np.int64) # Ensure labels are integers\n",
        "logging.info(f\"Created labels vector y with shape: {y.shape}\")\n",
        "unique_labels, counts = np.unique(y, return_counts=True)\n",
        "logging.info(f\"Label distribution in y: {dict(zip(unique_labels, counts))}\")\n",
        "\n",
        "\n",
        "# --- Clean up intermediate objects ---\n",
        "del flows_df_processed\n",
        "del scaled_numerical_df\n",
        "del categorical_encoded_df\n",
        "del X_features_list\n",
        "#del X_df # Remove intermediate dataframe\n",
        "gc.collect()\n",
        "logging.info(\"Feature engineering on sampled data complete. Produced feature matrix X and label vector y.\")\n",
        "\n",
        "# --- Display final shapes ---\n",
        "print(f\"Final Feature Matrix X shape: {X.shape}\")\n",
        "print(f\"Final Label Vector y shape: {y.shape}\")\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Define the save directory and file name\n",
        "save_dir = '/media/ssd/test/GNN/Standardized Models/CAGN-GAT/' # Relative path from notebook location might need adjustment\n",
        "# Or use an absolute path if easier:\n",
        "# save_dir = '/media/ssd/test/GNN/Adversarial Evaluation/' \n",
        "feature_list_filename = os.path.join(save_dir, 'cagn_gat_feature_list.pkl')\n",
        "\n",
        "# Ensure the directory exists (it should, but just in case)\n",
        "os.makedirs(save_dir, exist_ok=True) \n",
        "\n",
        "# Save the column names\n",
        "feature_list = X_df.columns.tolist()\n",
        "with open(feature_list_filename, 'wb') as f:\n",
        "    pickle.dump(feature_list, f)\n",
        "    \n",
        "print(f\"Saved {len(feature_list)} feature names to {feature_list_filename}\")\n",
        "# Optional: Clean up intermediate dataframe if memory is tight AFTER saving\n",
        "# del X_df \n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b063c556",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:37:07,049 - INFO - --- Starting Graph Construction Execution ---\n",
            "2025-04-12 23:37:07,051 - INFO - Starting adaptive graph construction for 49999 samples...\n",
            "2025-04-12 23:37:07,051 - INFO - Parameters: k=20, metric='euclidean', threshold=0.5\n",
            "2025-04-12 23:37:07,052 - INFO - Calculating k-NN graph...\n",
            "2025-04-12 23:37:12,843 - INFO - Calculated k-NN graph. Shape: (49999, 49999), NNZ: 999980. Time: 5.79s\n",
            "2025-04-12 23:37:12,846 - INFO - Calculating pairwise distances (using sklearn) and applying threshold...\n",
            "2025-04-12 23:37:36,990 - INFO - Calculated pairwise distances matrix. Shape: (49999, 49999). Time: 24.14s\n",
            "2025-04-12 23:37:39,220 - INFO - Created distance threshold adjacency mask.\n",
            "2025-04-12 23:37:54,851 - INFO - Converted distance mask to sparse CSR. NNZ: 73400740\n",
            "2025-04-12 23:37:55,082 - INFO - Intersecting k-NN (CSR) and distance threshold (CSR) graphs...\n",
            "2025-04-12 23:37:55,731 - INFO - Intersection complete. Final Adj NNZ: 954732. Time: 0.65s\n",
            "2025-04-12 23:37:55,741 - INFO - Final number of edges: 954732\n",
            "2025-04-12 23:37:55,751 - INFO - Created PyG Data object: Data(x=[49999, 365], edge_index=[2, 954732], y=[49999]). Time: 0.02s\n",
            "2025-04-12 23:37:55,887 - INFO - Cleaned up intermediate graph construction objects.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Graph construction successful. Total time: 48.84s\n",
            "Data(x=[49999, 365], edge_index=[2, 954732], y=[49999])\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Adaptive Graph Construction\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "from scipy import sparse # Need explicit import for sparse matrix operations\n",
        "import numpy as np\n",
        "import logging\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# Make sure X and y are available from Cell 2\n",
        "\n",
        "# --- Define Adaptive Graph Construction Function ---\n",
        "# (Based on the function found in the original CAGN-GAT Fusion notebook)\n",
        "def adaptive_graph_construction(X, y, k=20, adaptive_metric='euclidean', threshold=0.5):\n",
        "    \"\"\"\n",
        "    Adaptive graph construction based on feature similarity (k-NN intersection with distance threshold).\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): Feature matrix (num_samples, num_features).\n",
        "        y (np.ndarray): Labels (num_samples,).\n",
        "        k (int): Number of neighbors for k-NN graph.\n",
        "        adaptive_metric (str): Metric for feature similarity ('euclidean', 'cosine', etc.).\n",
        "        threshold (float): Threshold for edge creation based on similarity distance.\n",
        "\n",
        "    Returns:\n",
        "        Data: PyTorch Geometric Data object with nodes=samples, x=features, edge_index=similarity_edges.\n",
        "    \"\"\"\n",
        "    num_samples = X.shape[0]\n",
        "    logging.info(f\"Starting adaptive graph construction for {num_samples} samples...\")\n",
        "    logging.info(f\"Parameters: k={k}, metric='{adaptive_metric}', threshold={threshold}\")\n",
        "    start_time_knn = time.time()\n",
        "\n",
        "    # 1. Calculate k-NN graph adjacency\n",
        "    logging.info(\"Calculating k-NN graph...\")\n",
        "    # Note: kneighbors_graph returns a sparse matrix in CSR format by default\n",
        "    try:\n",
        "        knn_adj = kneighbors_graph(X, k, mode='connectivity', metric=adaptive_metric, include_self=False, n_jobs=-1) # Use all available CPU cores\n",
        "        logging.info(f\"Calculated k-NN graph. Shape: {knn_adj.shape}, NNZ: {knn_adj.nnz}. Time: {time.time() - start_time_knn:.2f}s\")\n",
        "    except MemoryError as e:\n",
        "        logging.error(f\"MemoryError during k-NN graph calculation: {e}. Consider reducing k or features.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during k-NN graph calculation: {e}\")\n",
        "        raise\n",
        "\n",
        "    # 2. Calculate pairwise distances and threshold\n",
        "    start_time_dist = time.time()\n",
        "    logging.info(\"Calculating pairwise distances (using sklearn) and applying threshold...\")\n",
        "    # This is the most memory-intensive part.\n",
        "    try:\n",
        "        # Calculate distances - use float32 for potentially lower memory usage if precision allows\n",
        "        # Note: pairwise_distances returns a dense matrix.\n",
        "        distances = pairwise_distances(X.astype(np.float32), metric=adaptive_metric, n_jobs=-1)\n",
        "        logging.info(f\"Calculated pairwise distances matrix. Shape: {distances.shape}. Time: {time.time() - start_time_dist:.2f}s\")\n",
        "\n",
        "        # Create adjacency matrix based on distance threshold\n",
        "        # This creates a dense boolean matrix initially\n",
        "        dist_adj_mask = (distances < threshold)\n",
        "        # Ensure it's boolean, set diagonal to False (no self-loops from distance)\n",
        "        np.fill_diagonal(dist_adj_mask, False)\n",
        "        logging.info(\"Created distance threshold adjacency mask.\")\n",
        "\n",
        "        # Convert boolean mask to sparse CSR format for efficient intersection\n",
        "        dist_adj_sparse = sparse.csr_matrix(dist_adj_mask)\n",
        "        logging.info(f\"Converted distance mask to sparse CSR. NNZ: {dist_adj_sparse.nnz}\")\n",
        "\n",
        "        # Cleanup large dense matrices asap\n",
        "        del distances\n",
        "        del dist_adj_mask\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "    except MemoryError as e:\n",
        "         logging.error(f\"MemoryError during pairwise distance calculation or conversion: {e}. Graph construction failed. Consider reducing features or using sampling.\")\n",
        "         raise\n",
        "    except Exception as e:\n",
        "         logging.error(f\"Error during distance calculation/thresholding: {e}\")\n",
        "         raise\n",
        "\n",
        "    # 3. Intersect k-NN graph and distance threshold graph\n",
        "    start_time_intersect = time.time()\n",
        "    logging.info(\"Intersecting k-NN (CSR) and distance threshold (CSR) graphs...\")\n",
        "    # Element-wise multiplication of sparse matrices performs intersection\n",
        "    # Both knn_adj and dist_adj_sparse should be CSR for this to be efficient\n",
        "    try:\n",
        "        final_adj = knn_adj.multiply(dist_adj_sparse)\n",
        "        logging.info(f\"Intersection complete. Final Adj NNZ: {final_adj.nnz}. Time: {time.time() - start_time_intersect:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during sparse matrix intersection: {e}\")\n",
        "        raise\n",
        "\n",
        "    # 4. Create PyG Data Object\n",
        "    start_time_pyg = time.time()\n",
        "    # Get final edge index in COO format for PyG\n",
        "    final_adj_coo = final_adj.tocoo()\n",
        "    edge_index = torch.tensor(np.vstack((final_adj_coo.row, final_adj_coo.col)), dtype=torch.long)\n",
        "    logging.info(f\"Final number of edges: {edge_index.shape[1]}\")\n",
        "\n",
        "    # Convert features and labels to tensors\n",
        "    # Use float32 for features to save memory\n",
        "    features = torch.tensor(X, dtype=torch.float32)\n",
        "    labels = torch.tensor(y, dtype=torch.long) # y should already be int64 from cell 2\n",
        "\n",
        "    data = Data(x=features, edge_index=edge_index, y=labels)\n",
        "    logging.info(f\"Created PyG Data object: {data}. Time: {time.time() - start_time_pyg:.2f}s\")\n",
        "\n",
        "    # Clean up remaining large intermediate objects\n",
        "    del knn_adj, dist_adj_sparse, final_adj, final_adj_coo\n",
        "    gc.collect()\n",
        "    logging.info(\"Cleaned up intermediate graph construction objects.\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# --- Build the Graph ---\n",
        "# Assuming X and y are numpy arrays from the previous cell\n",
        "\n",
        "# Build the graph using the function\n",
        "# This might take time and significant memory!\n",
        "logging.info(\"--- Starting Graph Construction Execution ---\")\n",
        "graph_build_start_time = time.time()\n",
        "try:\n",
        "    # Ensure y is numpy array when passed\n",
        "    data = adaptive_graph_construction(X, y, k=20, threshold=0.5)\n",
        "    print(f\"Graph construction successful. Total time: {time.time() - graph_build_start_time:.2f}s\")\n",
        "    print(data)\n",
        "except MemoryError:\n",
        "    print(\"\\n\"+\"=\"*20 + \" MEMORY ERROR \" + \"=\"*20)\n",
        "    print(\"MemoryError occurred during graph construction!\")\n",
        "    print(\"This is likely due to the high dimensionality of X (from OneHot)\")\n",
        "    print(\"or the large number of samples during distance calculation.\")\n",
        "    print(\"Consider falling back to LabelEncoding for categorical features\")\n",
        "    print(\"or reducing the number of samples if possible.\")\n",
        "    print(\"=\"*52)\n",
        "    # Optionally: Set data to None or raise the error again\n",
        "    data = None\n",
        "    # Raise the error to stop notebook execution if preferred\n",
        "    # raise\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn unexpected error occurred during graph construction: {e}\")\n",
        "    data = None\n",
        "    # raise\n",
        "\n",
        "# Note: If successful, the 'data' object now holds the graph structure.\n",
        "# Nodes represent the original flows/samples.\n",
        "# data.x holds the feature matrix X.\n",
        "# data.edge_index represents edges between samples based on feature similarity.\n",
        "# data.y holds the labels.\n",
        "# There is no data.edge_attr.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a82e9cbc",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:37:55,904 - INFO - Splitting nodes into training, validation, and test sets...\n",
            "2025-04-12 23:37:55,923 - INFO - Split complete:\n",
            "2025-04-12 23:37:55,924 - INFO -   Train nodes: 34999\n",
            "2025-04-12 23:37:55,925 - INFO -   Validation nodes: 7500\n",
            "2025-04-12 23:37:55,925 - INFO -   Test nodes: 7500\n",
            "2025-04-12 23:37:55,927 - INFO - Added train_mask, val_mask, test_mask to the Data object (Node-based).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data object updated with node masks:\n",
            "Data(x=[49999, 365], edge_index=[2, 954732], y=[49999], train_mask=[49999], val_mask=[49999], test_mask=[49999])\n",
            "Number of training nodes: 34999\n",
            "Number of validation nodes: 7500\n",
            "Number of test nodes: 7500\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cell 4: Data Splitting (Node-based for Sampled Graph)\n",
        "import torch\n",
        "import numpy as np\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Make sure the 'data' object is available from Cell 3\n",
        "if 'data' not in locals() or data is None:\n",
        "    logging.error(\"Data object 'data' not available. Cannot proceed with splitting.\")\n",
        "    raise NameError(\"Data object 'data' is required but not available.\")\n",
        "\n",
        "# --- 5. Data Splitting (Node Splitting) ---\n",
        "logging.info(\"Splitting nodes into training, validation, and test sets...\")\n",
        "\n",
        "num_nodes = data.num_nodes\n",
        "node_indices = np.arange(num_nodes)\n",
        "labels = data.y.cpu().numpy() # Use labels for stratified split\n",
        "\n",
        "# Define split proportions\n",
        "train_ratio = 0.70\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15 # Should sum to 1.0\n",
        "\n",
        "# Ensure ratios sum to 1 (or close enough)\n",
        "assert np.isclose(train_ratio + val_ratio + test_ratio, 1.0), \"Split ratios must sum to 1.0\"\n",
        "\n",
        "# First split into train and temp (val + test), stratifying by labels\n",
        "# Stratification helps maintain class distribution in splits, especially important after sampling\n",
        "try:\n",
        "    train_indices, temp_indices, y_train, y_temp = train_test_split(\n",
        "        node_indices,\n",
        "        labels, # Use labels for stratification\n",
        "        train_size=train_ratio,\n",
        "        random_state=42, # for reproducibility\n",
        "        stratify=labels # Stratify based on node labels\n",
        "    )\n",
        "except ValueError as e:\n",
        "    logging.warning(f\"Could not stratify during train/temp split (perhaps too few samples in a class?): {e}. Proceeding without stratification.\")\n",
        "    train_indices, temp_indices = train_test_split(\n",
        "        node_indices,\n",
        "        train_size=train_ratio,\n",
        "        random_state=42\n",
        "    )\n",
        "    y_temp = labels[temp_indices] # Need labels for the next stratification step\n",
        "\n",
        "\n",
        "# Calculate remaining proportion for validation relative to the temp set size\n",
        "# Adjust validation size calculation relative to the *remaining* data\n",
        "val_relative_ratio = val_ratio / (val_ratio + test_ratio)\n",
        "\n",
        "# Split temp into validation and test, stratifying by labels within the temp set\n",
        "try:\n",
        "    val_indices, test_indices, _, _ = train_test_split(\n",
        "        temp_indices,\n",
        "        y_temp, # Use labels from the temp set for stratification\n",
        "        train_size=val_relative_ratio,\n",
        "        random_state=42, # use same random_state for consistency\n",
        "        stratify=y_temp # Stratify within the temp set\n",
        "    )\n",
        "except ValueError as e:\n",
        "     logging.warning(f\"Could not stratify during val/test split: {e}. Proceeding without stratification.\")\n",
        "     val_indices, test_indices = train_test_split(\n",
        "        temp_indices,\n",
        "        train_size=val_relative_ratio,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "\n",
        "logging.info(f\"Split complete:\")\n",
        "logging.info(f\"  Train nodes: {len(train_indices)}\")\n",
        "logging.info(f\"  Validation nodes: {len(val_indices)}\")\n",
        "logging.info(f\"  Test nodes: {len(test_indices)}\")\n",
        "\n",
        "# Create boolean masks for nodes\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "\n",
        "train_mask[train_indices] = True\n",
        "val_mask[val_indices] = True\n",
        "test_mask[test_indices] = True\n",
        "\n",
        "# Add masks to the data object\n",
        "# Ensure masks are on the same device as the data object\n",
        "data.train_mask = train_mask.to(data.x.device)\n",
        "data.val_mask = val_mask.to(data.x.device)\n",
        "data.test_mask = test_mask.to(data.x.device)\n",
        "\n",
        "logging.info(\"Added train_mask, val_mask, test_mask to the Data object (Node-based).\")\n",
        "print(\"Data object updated with node masks:\")\n",
        "print(data)\n",
        "print(f\"Number of training nodes: {data.train_mask.sum().item()}\")\n",
        "print(f\"Number of validation nodes: {data.val_mask.sum().item()}\")\n",
        "print(f\"Number of test nodes: {data.test_mask.sum().item()}\")\n",
        "\n",
        "# Clean up intermediate numpy arrays\n",
        "del node_indices, labels, train_indices, temp_indices, val_indices, test_indices, y_train, y_temp\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e092ef1e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:37:56,086 - INFO - Defining CAGN model...\n",
            "2025-04-12 23:37:56,089 - INFO - Input dim: 365, Hidden dim: 128, Output dim: 1 (2 classes)\n",
            "2025-04-12 23:37:56,090 - INFO - Defined CAGN model.\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Model Definition (CAGN)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d, Dropout\n",
        "from torch_geometric.nn import GATConv, MessagePassing\n",
        "import logging\n",
        "\n",
        "# --- 6. Model Definition (CAGN) ---\n",
        "logging.info(\"Defining CAGN model...\")\n",
        "\n",
        "# Define input/output dimensions based on the 'data' object from Cell 3\n",
        "# Ensure 'data' exists and has the required attributes\n",
        "if 'data' not in locals() or data is None:\n",
        "    raise NameError(\"Data object 'data' is not available from graph construction.\")\n",
        "if not hasattr(data, 'num_node_features'):\n",
        "     raise AttributeError(\"Data object does not have 'num_node_features'. Graph construction might have failed.\")\n",
        "if not hasattr(data, 'y'):\n",
        "     raise AttributeError(\"Data object does not have 'y' (labels).\")\n",
        "\n",
        "\n",
        "node_feat_dim = data.num_node_features # Dimension of features in data.x\n",
        "hidden_dim = 128                       # Hyperparameter: Size of hidden layers\n",
        "# Determine output dimension based on labels (binary or multi-class)\n",
        "num_classes = len(torch.unique(data.y))\n",
        "output_dim = 1 if num_classes == 2 else num_classes # 1 for binary logits, num_classes for multi-class logits\n",
        "logging.info(f\"Input dim: {node_feat_dim}, Hidden dim: {hidden_dim}, Output dim: {output_dim} ({num_classes} classes)\")\n",
        "\n",
        "\n",
        "class CAGN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=8, dropout=0.6):\n",
        "        super(CAGN, self).__init__()\n",
        "        self.dropout_rate = dropout\n",
        "        # Use edge_dim=None since the similarity graph doesn't have edge features\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=self.dropout_rate, edge_dim=None)\n",
        "        # Input to conv2 is hidden_dim * heads because concat=True by default in GATConv\n",
        "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=self.dropout_rate, edge_dim=None)\n",
        "        # Input to conv3 is the output dimension of conv2, which is hidden_dim\n",
        "        self.conv3 = GATConv(hidden_dim, output_dim, heads=1, concat=False, dropout=self.dropout_rate, edge_dim=None)\n",
        "\n",
        "        # Projection head for contrastive loss (optional, but can sometimes help)\n",
        "        # Let's project the output of conv2 (which is hidden_dim)\n",
        "        # self.contrastive_proj = Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.contrastive_loss_weight = 0.5  # Hyperparameter: Balance between cls and contrastive loss\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x) # ELU activation is common after GAT layers\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # Output of conv2 is the embedding 'z' used for contrastive loss\n",
        "        z = self.conv2(x, edge_index)\n",
        "        z = F.elu(z)\n",
        "        z = F.dropout(z, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # Output of conv3 is the final classification logits\n",
        "        out_logits = self.conv3(z, edge_index)\n",
        "\n",
        "        # Return both the final logits and the intermediate embeddings\n",
        "        return out_logits, z\n",
        "\n",
        "    def contrastive_loss(self, z, labels, margin=1.0):\n",
        "        \"\"\"\n",
        "        Calculates contrastive loss on node embeddings 'z'.\n",
        "        Pulls nodes with the same label closer and pushes nodes with different labels apart.\n",
        "        \"\"\"\n",
        "        # Ensure labels are on the same device as embeddings\n",
        "        labels = labels.to(z.device)\n",
        "\n",
        "        # Normalize embeddings\n",
        "        norm_z = F.normalize(z, p=2, dim=1)\n",
        "\n",
        "        # Calculate cosine similarity matrix\n",
        "        sim_matrix = torch.mm(norm_z, norm_z.t())\n",
        "\n",
        "        # Create positive and negative masks based on labels\n",
        "        pos_mask = (labels.unsqueeze(1) == labels.unsqueeze(0)).float()\n",
        "        neg_mask = 1 - pos_mask\n",
        "\n",
        "        # Calculate loss for positive pairs (want similarity close to 1)\n",
        "        # We use (1 - sim) for positive pairs, aiming to minimize this distance\n",
        "        pos_loss = (1 - sim_matrix) * pos_mask\n",
        "\n",
        "        # Calculate loss for negative pairs (want similarity less than margin)\n",
        "        # We use relu(sim - margin), penalizing similarities > margin\n",
        "        neg_loss = F.relu(sim_matrix - margin) * neg_mask\n",
        "\n",
        "        # Average the losses over the number of pairs, avoiding division by zero\n",
        "        # Summing over all pairs (upper/lower triangle, excluding diagonal implicitly by masks)\n",
        "        num_pos_pairs = pos_mask.sum()\n",
        "        num_neg_pairs = neg_mask.sum()\n",
        "\n",
        "        pos_term = pos_loss.sum() / (num_pos_pairs + 1e-8) # Add epsilon for stability\n",
        "        neg_term = neg_loss.sum() / (num_neg_pairs + 1e-8) # Add epsilon for stability\n",
        "\n",
        "        # Combine positive and negative losses\n",
        "        total_contrastive_loss = pos_term + neg_term\n",
        "\n",
        "        return total_contrastive_loss\n",
        "\n",
        "\n",
        "logging.info(f\"Defined CAGN model.\")\n",
        "# Example instantiation (optional, for checking structure)\n",
        "# model_cagn = CAGN(node_feat_dim, hidden_dim, output_dim)\n",
        "# print(model_cagn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "97a89242",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:37:56,114 - INFO - Defining CAGN model...\n",
            "2025-04-12 23:37:56,117 - INFO - Input dim: 365, Hidden dim: 64, Output dim: 1 (2 classes)\n",
            "2025-04-12 23:37:56,118 - INFO - Defined CAGN model with batched contrastive loss.\n"
          ]
        }
      ],
      "source": [
        "# Cell 5: Model Definition (CAGN - With Batched Contrastive Loss)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d, Dropout\n",
        "from torch_geometric.nn import GATConv, MessagePassing\n",
        "import logging\n",
        "\n",
        "# --- 6. Model Definition (CAGN) ---\n",
        "logging.info(\"Defining CAGN model...\")\n",
        "\n",
        "# Define input/output dimensions based on the 'data' object from Cell 3\n",
        "# Ensure 'data' exists and has the required attributes\n",
        "if 'data' not in locals() or data is None:\n",
        "    raise NameError(\"Data object 'data' is not available from graph construction.\")\n",
        "if not hasattr(data, 'num_node_features'):\n",
        "     raise AttributeError(\"Data object does not have 'num_node_features'. Graph construction might have failed.\")\n",
        "if not hasattr(data, 'y'):\n",
        "     raise AttributeError(\"Data object does not have 'y' (labels).\")\n",
        "\n",
        "\n",
        "node_feat_dim = data.num_node_features # Dimension of features in data.x\n",
        "# Keep hidden_dim and heads reduced as they were causing issues before\n",
        "hidden_dim = 64                        # Hyperparameter: Size of hidden layers (Kept reduced)\n",
        "gat_heads = 8                          # Number of attention heads (Kept reduced)\n",
        "# Determine output dimension based on labels (binary or multi-class)\n",
        "num_classes = len(torch.unique(data.y))\n",
        "output_dim = 1 if num_classes == 2 else num_classes # 1 for binary logits, num_classes for multi-class logits\n",
        "logging.info(f\"Input dim: {node_feat_dim}, Hidden dim: {hidden_dim}, Output dim: {output_dim} ({num_classes} classes)\")\n",
        "\n",
        "\n",
        "class CAGN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=2, dropout=0.6): # Default heads=2\n",
        "        super(CAGN, self).__init__()\n",
        "        self.dropout_rate = dropout\n",
        "        # Use edge_dim=None since the similarity graph doesn't have edge features\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=self.dropout_rate, edge_dim=None)\n",
        "        # Input to conv2 is hidden_dim * heads because concat=True by default in GATConv\n",
        "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=1, concat=False, dropout=self.dropout_rate, edge_dim=None)\n",
        "        # Input to conv3 is the output dimension of conv2, which is hidden_dim\n",
        "        self.conv3 = GATConv(hidden_dim, output_dim, heads=1, concat=False, dropout=self.dropout_rate, edge_dim=None)\n",
        "\n",
        "        self.contrastive_loss_weight = 0.5  # Hyperparameter: Balance between cls and contrastive loss\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x) # ELU activation is common after GAT layers\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # Output of conv2 is the embedding 'z' used for contrastive loss\n",
        "        z = self.conv2(x, edge_index)\n",
        "        z = F.elu(z)\n",
        "        z = F.dropout(z, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # Output of conv3 is the final classification logits\n",
        "        out_logits = self.conv3(z, edge_index)\n",
        "\n",
        "        # Return both the final logits and the intermediate embeddings\n",
        "        return out_logits, z\n",
        "\n",
        "    def contrastive_loss(self, z, labels, margin=1.0, batch_size=1024):\n",
        "        \"\"\"\n",
        "        Calculates contrastive loss on node embeddings 'z' in batches.\n",
        "        Pulls nodes with the same label closer and pushes nodes with different labels apart.\n",
        "        \"\"\"\n",
        "        # Ensure labels are on the same device as embeddings\n",
        "        labels = labels.to(z.device)\n",
        "        num_nodes = z.size(0)\n",
        "\n",
        "        if num_nodes == 0:\n",
        "            return torch.tensor(0.0, device=z.device, requires_grad=True) # Handle empty input\n",
        "\n",
        "        # Normalize embeddings\n",
        "        norm_z = F.normalize(z, p=2, dim=1)\n",
        "\n",
        "        total_pos_loss = 0.0\n",
        "        total_neg_loss = 0.0\n",
        "        total_pos_pairs = 0.0\n",
        "        total_neg_pairs = 0.0\n",
        "\n",
        "        for i in range(0, num_nodes, batch_size):\n",
        "            # Select batch embeddings and labels\n",
        "            batch_indices = torch.arange(i, min(i + batch_size, num_nodes), device=z.device)\n",
        "            batch_z = norm_z[batch_indices]\n",
        "            batch_labels = labels[batch_indices]\n",
        "\n",
        "            # Compute similarity between batch and all nodes\n",
        "            sim_sub_matrix = torch.mm(batch_z, norm_z.t()) # Shape: [batch_size, num_nodes]\n",
        "\n",
        "            # Create positive and negative masks for the batch vs all nodes\n",
        "            pos_sub_mask = (batch_labels.unsqueeze(1) == labels.unsqueeze(0)).float() # Shape: [batch_size, num_nodes]\n",
        "            # Avoid comparing node to itself within the batch's slice of the full matrix\n",
        "            pos_sub_mask[torch.arange(batch_z.size(0)), batch_indices] = 0\n",
        "            neg_sub_mask = 1 - pos_sub_mask\n",
        "            # Also explicitly remove self-comparisons for negative mask, although usually covered by pos_mask\n",
        "            neg_sub_mask[torch.arange(batch_z.size(0)), batch_indices] = 0\n",
        "\n",
        "\n",
        "            # Calculate loss for this batch\n",
        "            batch_pos_loss = (1 - sim_sub_matrix) * pos_sub_mask\n",
        "            batch_neg_loss = F.relu(sim_sub_matrix - margin) * neg_sub_mask\n",
        "\n",
        "            # Accumulate losses and pair counts\n",
        "            total_pos_loss += batch_pos_loss.sum()\n",
        "            total_neg_loss += batch_neg_loss.sum()\n",
        "            total_pos_pairs += pos_sub_mask.sum()\n",
        "            total_neg_pairs += neg_sub_mask.sum()\n",
        "\n",
        "            # --- Memory cleanup within loop ---\n",
        "            del sim_sub_matrix, pos_sub_mask, neg_sub_mask, batch_pos_loss, batch_neg_loss\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            # --- End Memory cleanup ---\n",
        "\n",
        "\n",
        "        # Final average loss\n",
        "        pos_term = total_pos_loss / (total_pos_pairs + 1e-8) # Add epsilon for stability\n",
        "        neg_term = total_neg_loss / (total_neg_pairs + 1e-8) # Add epsilon for stability\n",
        "\n",
        "        # Check for NaN potential (if no positive or no negative pairs were found)\n",
        "        if torch.isnan(pos_term): pos_term = 0.0\n",
        "        if torch.isnan(neg_term): neg_term = 0.0\n",
        "\n",
        "        total_contrastive_loss = pos_term + neg_term\n",
        "\n",
        "        return total_contrastive_loss\n",
        "\n",
        "\n",
        "logging.info(f\"Defined CAGN model with batched contrastive loss.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0d3e39e7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:37:56,162 - INFO - Defined baseline GNN models: GCN, BaselineGAT, GraphSAGE, GIN.\n",
            "2025-04-12 23:37:56,163 - INFO - Setting up training and evaluation functions (Node Classification)...\n",
            "2025-04-12 23:37:56,164 - INFO - Using device: cuda\n",
            "2025-04-12 23:37:57,680 - INFO - Calculated pos_weight for BCEWithLogitsLoss (Train set): 1.0910 (on cuda:0)\n",
            "2025-04-12 23:37:57,682 - INFO - Classification loss: BCEWithLogitsLoss() initialized.\n",
            "2025-04-12 23:37:57,704 - INFO - Moved data object to cuda\n",
            "2025-04-12 23:37:57,705 - INFO - Unified training loop and evaluation functions defined.\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Training & Evaluation Setup (All Models)\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, roc_curve\n",
        "import logging\n",
        "import time\n",
        "import gc\n",
        "import numpy as np # Ensure numpy is imported\n",
        "# --- Baseline Model Imports ---\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, GINConv # Import necessary layers\n",
        "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d\n",
        "import torch.nn.functional as F # Ensure F is imported\n",
        "# --- End Baseline Model Imports ---\n",
        "\n",
        "\n",
        "# --- Baseline GNN Model Definitions ---\n",
        "class GCN(torch.nn.Module):\n",
        "    \"\"\"Standard GCN model for baseline comparison.\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training) # Add dropout\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training) # Add dropout\n",
        "        x = self.conv3(x, edge_index) # Return logits\n",
        "        return x\n",
        "\n",
        "class BaselineGAT(torch.nn.Module): # Renamed to avoid conflict if CAGN uses GAT\n",
        "    \"\"\"Standard GAT model for baseline comparison.\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=8, dropout=0.6):\n",
        "        super(BaselineGAT, self).__init__()\n",
        "        # Ensure hidden_dim is reasonable for multiplication by heads\n",
        "        # Find the largest multiple of heads <= hidden_dim\n",
        "        gat_hidden_dim_per_head = hidden_dim // heads\n",
        "        gat_hidden_dim = gat_hidden_dim_per_head * heads\n",
        "        if gat_hidden_dim == 0: # Handle case where hidden_dim < heads\n",
        "             gat_hidden_dim_per_head = 1\n",
        "             gat_hidden_dim = heads\n",
        "             logging.warning(f\"GAT hidden_dim ({hidden_dim}) < heads ({heads}). Setting hidden per head to 1.\")\n",
        "\n",
        "\n",
        "        if gat_hidden_dim != hidden_dim:\n",
        "            logging.warning(f\"Adjusting GAT hidden_dim from {hidden_dim} to {gat_hidden_dim} to be divisible by heads={heads}\")\n",
        "\n",
        "\n",
        "        self.conv1 = GATConv(input_dim, gat_hidden_dim_per_head, heads=heads, dropout=dropout, concat=True)\n",
        "        # Input to conv2 is the concatenated output: gat_hidden_dim\n",
        "        self.conv2 = GATConv(gat_hidden_dim, gat_hidden_dim_per_head, heads=heads, concat=True, dropout=dropout)\n",
        "        # Input to conv3 is the concatenated output: gat_hidden_dim\n",
        "        self.conv3 = GATConv(gat_hidden_dim, output_dim, heads=1, concat=False, dropout=dropout) # Final layer output dim is correct\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.6, training=self.training) # Match dropout used in GATConv\n",
        "        x = F.elu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv3(x, edge_index) # Return logits\n",
        "        return x\n",
        "\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    \"\"\"Standard GraphSAGE model for baseline comparison.\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = SAGEConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv3(x, edge_index) # Return logits\n",
        "        return x\n",
        "\n",
        "class GIN(torch.nn.Module):\n",
        "    \"\"\"Graph Isomorphism Network (GIN) model for baseline comparison.\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GIN, self).__init__()\n",
        "        nn1 = Sequential(Linear(input_dim, hidden_dim), ReLU(), Linear(hidden_dim, hidden_dim))\n",
        "        self.conv1 = GINConv(nn1)\n",
        "        self.bn1 = BatchNorm1d(hidden_dim)\n",
        "\n",
        "        nn2 = Sequential(Linear(hidden_dim, hidden_dim), ReLU(), Linear(hidden_dim, hidden_dim))\n",
        "        self.conv2 = GINConv(nn2)\n",
        "        self.bn2 = BatchNorm1d(hidden_dim)\n",
        "\n",
        "        nn3 = Sequential(Linear(hidden_dim, hidden_dim), ReLU(), Linear(hidden_dim, hidden_dim))\n",
        "        self.conv3 = GINConv(nn3)\n",
        "        self.bn3 = BatchNorm1d(hidden_dim)\n",
        "\n",
        "        self.fc = Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(self.conv2(x, edge_index))\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(self.conv3(x, edge_index))\n",
        "        x = self.bn3(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.fc(x) # Return logits\n",
        "        return x\n",
        "\n",
        "logging.info(\"Defined baseline GNN models: GCN, BaselineGAT, GraphSAGE, GIN.\")\n",
        "# --- End Baseline Model Definitions ---\n",
        "\n",
        "\n",
        "# Make sure 'data' object (with masks) and 'CAGN' model class are available\n",
        "if 'data' not in locals() or data is None:\n",
        "    raise NameError(\"Data object 'data' is required but not available.\")\n",
        "if 'CAGN' not in globals():\n",
        "      raise NameError(\"CAGN model class not defined. Ensure Cell 5 executed successfully.\")\n",
        "if not hasattr(data, 'train_mask'):\n",
        "     raise AttributeError(\"Data object missing 'train_mask'. Ensure node splitting (Cell 4) executed successfully.\")\n",
        "\n",
        "# --- 7. Training & Evaluation Setup ---\n",
        "logging.info(\"Setting up training and evaluation functions (Node Classification)...\") # Renamed log message\n",
        "\n",
        "# --- 7.1 Device Configuration ---\n",
        "# Determine device early\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logging.info(f\"Using device: {device}\")\n",
        "\n",
        "# --- 7.2 Loss Functions ---\n",
        "# Determine if binary or multi-class based on output_dim derived from data.y\n",
        "# Re-calculate based on data object just in case\n",
        "num_classes_check = len(torch.unique(data.y))\n",
        "output_dim_check = 1 if num_classes_check == 2 else num_classes_check\n",
        "is_binary = (output_dim_check == 1)\n",
        "\n",
        "if is_binary:\n",
        "    # Calculate positive class weight for handling imbalance (using node labels)\n",
        "    num_nodes = data.num_nodes\n",
        "    train_labels = data.y[data.train_mask]\n",
        "    num_positives = (train_labels == 1).sum().item()\n",
        "    num_negatives = (train_labels == 0).sum().item()\n",
        "    pos_weight = None\n",
        "    if num_positives > 0 and num_negatives > 0:\n",
        "        pos_weight_value = num_negatives / num_positives\n",
        "        pos_weight = torch.tensor([pos_weight_value], device=device)\n",
        "        logging.info(f\"Calculated pos_weight for BCEWithLogitsLoss (Train set): {pos_weight_value:.4f} (on {pos_weight.device})\")\n",
        "    else:\n",
        "        logging.warning(\"Could not calculate pos_weight for train set (num_positives or num_negatives is zero). Using default weighting.\")\n",
        "\n",
        "    criterion_cls = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "    logging.info(f\"Classification loss: {criterion_cls} initialized.\")\n",
        "else:\n",
        "    # No need for pos_weight for multi-class CrossEntropyLoss\n",
        "    # Weighting can be added via the 'weight' argument if needed\n",
        "    criterion_cls = nn.CrossEntropyLoss()\n",
        "    logging.info(f\"Classification loss: {criterion_cls} initialized for multi-class.\")\n",
        "\n",
        "\n",
        "# Contrastive loss is handled by the CAGN model's method\n",
        "\n",
        "# --- 7.3 Unified Training Function (Single Epoch) ---\n",
        "def train_epoch(model, data, optimizer, criterion_cls, is_cagn_model=False):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Perform forward pass\n",
        "    if is_cagn_model:\n",
        "        out_logits, z_embeddings = model(data.x, data.edge_index)\n",
        "    else:\n",
        "        # Baseline models return only logits\n",
        "        out_logits = model(data.x, data.edge_index)\n",
        "\n",
        "    # Squeeze logits if necessary (especially for binary classification outputting [N, 1])\n",
        "    logits_train = out_logits[data.train_mask]\n",
        "    if is_binary and logits_train.ndim > 1 and logits_train.shape[1] == 1:\n",
        "        logits_train = logits_train.squeeze(1)\n",
        "    elif logits_train.ndim == 0: # Handle case where mask selects only one sample\n",
        "         logits_train = logits_train.unsqueeze(0)\n",
        "\n",
        "    # Prepare labels\n",
        "    labels_train = data.y[data.train_mask]\n",
        "    target_labels = labels_train.float() if is_binary else labels_train.long()\n",
        "\n",
        "    # Calculate classification loss on training nodes\n",
        "    cls_loss = criterion_cls(logits_train, target_labels)\n",
        "\n",
        "    # Calculate contrastive loss ONLY for CAGN\n",
        "    contrast_loss = torch.tensor(0.0, device=cls_loss.device) # Default to 0\n",
        "    if is_cagn_model:\n",
        "        contrast_loss = model.contrastive_loss(z_embeddings[data.train_mask], data.y[data.train_mask])\n",
        "        total_loss = cls_loss + model.contrastive_loss_weight * contrast_loss\n",
        "    else:\n",
        "        total_loss = cls_loss # Total loss is just classification loss for baselines\n",
        "\n",
        "    # Backpropagation\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Return individual losses for logging\n",
        "    return total_loss.item(), cls_loss.item(), contrast_loss.item()\n",
        "\n",
        "\n",
        "# --- 7.4 Unified Evaluation Function ---\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data, mask, criterion_cls, is_cagn_model=False):\n",
        "    model.eval()\n",
        "\n",
        "    # Perform forward pass\n",
        "    if is_cagn_model:\n",
        "        out_logits, z_embeddings = model(data.x, data.edge_index)\n",
        "    else:\n",
        "        out_logits = model(data.x, data.edge_index)\n",
        "        z_embeddings = None # Not needed for baselines\n",
        "\n",
        "    # Calculate classification loss on the specified mask\n",
        "    logits_eval = out_logits[mask]\n",
        "    if is_binary and logits_eval.ndim > 1 and logits_eval.shape[1] == 1:\n",
        "         logits_eval = logits_eval.squeeze(1)\n",
        "    elif logits_eval.ndim == 0: # Handle case where mask selects only one sample\n",
        "         logits_eval = logits_eval.unsqueeze(0)\n",
        "\n",
        "    labels_eval = data.y[mask]\n",
        "    target_labels_eval = labels_eval.float() if is_binary else labels_eval.long()\n",
        "\n",
        "    cls_loss = criterion_cls(logits_eval, target_labels_eval).item()\n",
        "\n",
        "    # Contrastive loss calculation during evaluation is often skipped or handled differently\n",
        "    # For simplicity, we report it as 0 here for evaluation metrics.\n",
        "    contrast_loss = 0.0\n",
        "    # if is_cagn_model and z_embeddings is not None:\n",
        "        # You *could* calculate it, but it adds overhead to evaluation\n",
        "        # contrast_loss = model.contrastive_loss(z_embeddings[mask], data.y[mask]).item()\n",
        "\n",
        "    total_loss = cls_loss + (model.contrastive_loss_weight * contrast_loss if is_cagn_model else 0.0)\n",
        "\n",
        "    # Get predictions\n",
        "    if is_binary:\n",
        "        preds_proba = torch.sigmoid(logits_eval) # Probabilities for binary\n",
        "        preds = (preds_proba > 0.5).float()            # Threshold at 0.5\n",
        "    else:\n",
        "        preds_proba = F.softmax(logits_eval, dim=-1) # Probabilities for multi-class\n",
        "        preds = preds_proba.argmax(dim=-1)             # Get class with highest probability\n",
        "\n",
        "    # Get ground truth labels\n",
        "    labels = data.y[mask]\n",
        "\n",
        "    # Ensure labels and preds are on CPU for sklearn\n",
        "    labels_cpu = labels.cpu().numpy()\n",
        "    preds_cpu = preds.cpu().numpy()\n",
        "    preds_proba_cpu = preds_proba.cpu().numpy()\n",
        "\n",
        "    # --- Calculate Metrics ---\n",
        "    accuracy = accuracy_score(labels_cpu, preds_cpu)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels_cpu, preds_cpu, average='binary' if is_binary else 'macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    auc = 0.0\n",
        "    # Ensure there are samples to evaluate and more than one class in the labels\n",
        "    if mask.sum() > 0 and len(np.unique(labels_cpu)) > 1:\n",
        "        try:\n",
        "            if is_binary:\n",
        "                # Use probabilities of the positive class (usually index 1 if output is [N] or index 0 if [N,1])\n",
        "                proba_positive_class = preds_proba_cpu if preds_proba_cpu.ndim == 1 else preds_proba_cpu[:, 0]\n",
        "                if len(np.unique(labels_cpu)) > 1: # Check again within binary case\n",
        "                     auc = roc_auc_score(labels_cpu, proba_positive_class)\n",
        "                else:\n",
        "                     logging.warning(f\"Skipping AUC: Only one class present in labels for the current mask.\")\n",
        "            elif preds_proba_cpu.ndim > 1 and preds_proba_cpu.shape[1] >= 2: # Check if enough classes for multi-class AUC\n",
        "                auc = roc_auc_score(labels_cpu, preds_proba_cpu, multi_class='ovr', average='macro')\n",
        "            else:\n",
        "                 logging.warning(\"AUC calculation skipped: Not enough classes or incompatible probability shapes.\")\n",
        "        except ValueError as e:\n",
        "            logging.warning(f\"Could not calculate AUC: {e}. Setting AUC to 0.0\")\n",
        "            auc = 0.0\n",
        "    else:\n",
        "        # Log why AUC is skipped. Add check for mask.sum() > 0\n",
        "        if mask.sum() == 0:\n",
        "             logging.warning(f\"AUC calculation skipped: No samples in the current mask.\")\n",
        "        else:\n",
        "             logging.warning(f\"AUC calculation skipped: Only one class ({np.unique(labels_cpu)}) present in labels for the current mask ({mask.sum()} samples).\")\n",
        "\n",
        "\n",
        "    metrics = {\n",
        "        'loss': total_loss,\n",
        "        'cls_loss': cls_loss,\n",
        "        'contrast_loss': contrast_loss, # Will be 0 for baselines and potentially 0 for CAGN eval here\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "# --- 7.5 Device Configuration (Move data object) ---\n",
        "try:\n",
        "    if data.x.device != device:\n",
        "         data = data.to(device)\n",
        "         logging.info(f\"Moved data object to {device}\")\n",
        "    else:\n",
        "         logging.info(f\"Data object already on {device}\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Failed to move data to {device}: {e}. Check GPU memory. Falling back to CPU.\")\n",
        "    device = torch.device('cpu')\n",
        "    data = data.to(device)\n",
        "    # Re-initialize binary classification criterion with pos_weight on CPU if needed\n",
        "    if is_binary and pos_weight is not None and pos_weight.device != device:\n",
        "        pos_weight = pos_weight.to(device)\n",
        "        criterion_cls = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "        logging.info(f\"Fallback: Moved pos_weight back to {device} and re-initialized criterion_cls\")\n",
        "\n",
        "logging.info(\"Unified training loop and evaluation functions defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "570d0441",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:37:58,070 - INFO - Starting processing for dataset: Combined_10pct from /media/ssd/test/standardized-datasets/combined/combined_unsw_cicRed_botRed_netflow_10pct.csv\n",
            "2025-04-12 23:37:58,071 - INFO - Loading dataset: Combined_10pct\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Processing Dataset: Combined_10pct\n",
            "==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:38:01,560 - INFO - Dataset Combined_10pct loaded. Shape: (655094, 47)\n",
            "2025-04-12 23:38:01,564 - INFO - Starting sampling for Combined_10pct...\n",
            "2025-04-12 23:38:01,564 - INFO - Starting imbalanced sampling. Target size for large classes: 50000\n",
            "2025-04-12 23:38:01,571 - INFO - Original class distribution:\\nLabel\n",
            "0    341806\n",
            "1    313288\n",
            "Name: count, dtype: int64\n",
            "2025-04-12 23:38:01,572 - INFO - Found 2 large classes (>= 1000 samples).\n",
            "2025-04-12 23:38:01,573 - INFO - Scaling factor for large classes: 0.0763\n",
            "2025-04-12 23:38:01,573 - INFO -   Sampling class '0': target size=26088, final sample size=26088\n",
            "2025-04-12 23:38:01,648 - INFO -   Sampling class '1': target size=23911, final sample size=23911\n",
            "2025-04-12 23:38:01,743 - INFO - Finished sampling. New dataset size: 49999\n",
            "2025-04-12 23:38:01,745 - INFO - New class distribution:\\nLabel\n",
            "0    26088\n",
            "1    23911\n",
            "Name: count, dtype: int64\n",
            "2025-04-12 23:38:01,806 - INFO - Sampling complete for Combined_10pct. Sampled shape: (49999, 47)\n",
            "2025-04-12 23:38:02,011 - INFO - Starting feature engineering for Combined_10pct...\n",
            "2025-04-12 23:38:02,558 - INFO - Feature engineering complete for Combined_10pct. X shape: (49999, 365), y shape: (49999,)\n",
            "2025-04-12 23:38:02,716 - INFO - Starting adaptive graph construction for Combined_10pct...\n",
            "2025-04-12 23:38:02,716 - INFO - Starting adaptive graph construction for 49999 samples...\n",
            "2025-04-12 23:38:02,717 - INFO - Parameters: k=20, metric='euclidean', threshold=0.5\n",
            "2025-04-12 23:38:02,717 - INFO - Calculating k-NN graph...\n",
            "2025-04-12 23:38:08,264 - INFO - Calculated k-NN graph. Shape: (49999, 49999), NNZ: 999980. Time: 5.55s\n",
            "2025-04-12 23:38:08,265 - INFO - Calculating pairwise distances (using sklearn) and applying threshold...\n",
            "2025-04-12 23:38:30,281 - INFO - Calculated pairwise distances matrix. Shape: (49999, 49999). Time: 22.01s\n",
            "2025-04-12 23:38:32,014 - INFO - Created distance threshold adjacency mask.\n",
            "2025-04-12 23:38:47,746 - INFO - Converted distance mask to sparse CSR. NNZ: 73400740\n",
            "2025-04-12 23:38:48,217 - INFO - Intersecting k-NN (CSR) and distance threshold (CSR) graphs...\n",
            "2025-04-12 23:38:48,885 - INFO - Intersection complete. Final Adj NNZ: 954732. Time: 0.67s\n",
            "2025-04-12 23:38:48,896 - INFO - Final number of edges: 954732\n",
            "2025-04-12 23:38:48,909 - INFO - Created PyG Data object: Data(x=[49999, 365], edge_index=[2, 954732], y=[49999]). Time: 0.02s\n",
            "2025-04-12 23:38:49,081 - INFO - Cleaned up intermediate graph construction objects.\n",
            "2025-04-12 23:38:49,083 - INFO - Graph construction complete for Combined_10pct. Data object: Data(x=[49999, 365], edge_index=[2, 954732], y=[49999])\n",
            "2025-04-12 23:38:49,232 - INFO - Creating train/val/test masks for Combined_10pct...\n",
            "2025-04-12 23:38:49,251 - INFO - Masks created for Combined_10pct. Train: 34999, Val: 7500, Test: 7500\n",
            "2025-04-12 23:38:49,433 - INFO - Moved data for Combined_10pct to cuda\n",
            "2025-04-12 23:38:49,643 - INFO - Starting training for CAGN on Combined_10pct\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training CAGN for Combined_10pct ---\n",
            "    Epoch: 001, Train Loss: 1.4275 (CLS: 0.9477, Contr: 0.9596), Val Loss: 0.6366 (CLS: 0.6366), Val F1: 0.7599, Val AUC: 0.8789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:39:18,956 - INFO - Early stopping for CAGN on Combined_10pct at epoch 43.\n",
            "2025-04-12 23:39:18,996 - INFO - Saved best CAGN model state for Combined_10pct to /media/ssd/test/GNN/Standardized Models/CAGN-GAT/best_cagn_model_Combined_10pct.pt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Test F1 for CAGN: 0.8233, AUC: 0.9582\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:39:19,264 - INFO - Starting training for GCN on Combined_10pct\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training GCN for Combined_10pct ---\n",
            "    Epoch: 001, Train Loss: 0.7043 (CLS: 0.7043), Val Loss: 0.6891 (CLS: 0.6891), Val F1: 0.7305, Val AUC: 0.8326\n",
            "    Epoch: 050, Train Loss: 0.2839 (CLS: 0.2839), Val Loss: 0.2642 (CLS: 0.2642), Val F1: 0.9187, Val AUC: 0.9630\n",
            "    Epoch: 100, Train Loss: 0.2162 (CLS: 0.2162), Val Loss: 0.2062 (CLS: 0.2062), Val F1: 0.9403, Val AUC: 0.9723\n",
            "    Epoch: 150, Train Loss: 0.1927 (CLS: 0.1927), Val Loss: 0.1885 (CLS: 0.1885), Val F1: 0.9432, Val AUC: 0.9755\n",
            "    Epoch: 200, Train Loss: 0.1784 (CLS: 0.1784), Val Loss: 0.1789 (CLS: 0.1789), Val F1: 0.9480, Val AUC: 0.9771\n",
            "    Test F1 for GCN: 0.9433, AUC: 0.9770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:39:24,373 - INFO - Starting training for BaselineGAT on Combined_10pct\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training BaselineGAT for Combined_10pct ---\n",
            "    Epoch: 001, Train Loss: 0.8349 (CLS: 0.8349), Val Loss: 0.7243 (CLS: 0.7243), Val F1: 0.3211, Val AUC: 0.5115\n",
            "    Epoch: 050, Train Loss: 0.3961 (CLS: 0.3961), Val Loss: 0.2809 (CLS: 0.2809), Val F1: 0.9169, Val AUC: 0.9598\n",
            "    Epoch: 100, Train Loss: 0.3362 (CLS: 0.3362), Val Loss: 0.2257 (CLS: 0.2257), Val F1: 0.9309, Val AUC: 0.9698\n",
            "    Epoch: 150, Train Loss: 0.3244 (CLS: 0.3244), Val Loss: 0.2151 (CLS: 0.2151), Val F1: 0.9341, Val AUC: 0.9698\n",
            "    Epoch: 200, Train Loss: 0.3179 (CLS: 0.3179), Val Loss: 0.2126 (CLS: 0.2126), Val F1: 0.9345, Val AUC: 0.9704\n",
            "    Test F1 for BaselineGAT: 0.9290, AUC: 0.9725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:39:31,902 - INFO - Starting training for GraphSAGE on Combined_10pct\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training GraphSAGE for Combined_10pct ---\n",
            "    Epoch: 001, Train Loss: 0.7214 (CLS: 0.7214), Val Loss: 0.7140 (CLS: 0.7140), Val F1: 0.7014, Val AUC: 0.7901\n",
            "    Epoch: 050, Train Loss: 0.3043 (CLS: 0.3043), Val Loss: 0.2964 (CLS: 0.2964), Val F1: 0.9019, Val AUC: 0.9562\n",
            "    Epoch: 100, Train Loss: 0.1936 (CLS: 0.1936), Val Loss: 0.1872 (CLS: 0.1872), Val F1: 0.9460, Val AUC: 0.9760\n",
            "    Epoch: 150, Train Loss: 0.1561 (CLS: 0.1561), Val Loss: 0.1555 (CLS: 0.1555), Val F1: 0.9539, Val AUC: 0.9823\n",
            "    Epoch: 200, Train Loss: 0.1380 (CLS: 0.1380), Val Loss: 0.1415 (CLS: 0.1415), Val F1: 0.9579, Val AUC: 0.9850\n",
            "    Test F1 for GraphSAGE: 0.9554, AUC: 0.9855\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:39:42,106 - INFO - Starting training for GIN on Combined_10pct\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training GIN for Combined_10pct ---\n",
            "    Epoch: 001, Train Loss: 0.7936 (CLS: 0.7936), Val Loss: 0.9884 (CLS: 0.9884), Val F1: 0.0055, Val AUC: 0.4501\n",
            "    Epoch: 050, Train Loss: 0.3150 (CLS: 0.3150), Val Loss: 0.3455 (CLS: 0.3455), Val F1: 0.8190, Val AUC: 0.9419\n",
            "    Epoch: 100, Train Loss: 0.1472 (CLS: 0.1472), Val Loss: 0.2733 (CLS: 0.2733), Val F1: 0.9595, Val AUC: 0.9822\n",
            "    Epoch: 150, Train Loss: 0.1028 (CLS: 0.1028), Val Loss: 0.1139 (CLS: 0.1139), Val F1: 0.9652, Val AUC: 0.9925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:39:52,689 - INFO - Early stopping for GIN on Combined_10pct at epoch 195.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Test F1 for GIN: 0.9650, AUC: 0.9933\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:39:53,088 - INFO - Starting processing for dataset: CICIDS2018_Reduced from /media/ssd/test/standardized-datasets/netflow/nf_cic_ids2018_reduced_standardized.csv\n",
            "2025-04-12 23:39:53,089 - INFO - Loading dataset: CICIDS2018_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for Combined_10pct ---\n",
            "| Model       |   Accuracy |   Precision |   Recall |     F1 |    AUC |   Test Loss (CLS) |   Training Time (s) |\n",
            "|:------------|-----------:|------------:|---------:|-------:|-------:|------------------:|--------------------:|\n",
            "| GIN         |     0.9669 |      0.976  |   0.9543 | 0.965  | 0.9933 |            0.1036 |             10.5824 |\n",
            "| GraphSAGE   |     0.9579 |      0.9669 |   0.9442 | 0.9554 | 0.9855 |            0.1432 |              9.9704 |\n",
            "| GCN         |     0.9469 |      0.9645 |   0.9231 | 0.9433 | 0.977  |            0.186  |              4.8885 |\n",
            "| BaselineGAT |     0.9321 |      0.9296 |   0.9284 | 0.929  | 0.9725 |            0.2105 |              7.3028 |\n",
            "| CAGN        |     0.8507 |      0.9484 |   0.7273 | 0.8233 | 0.9582 |            0.3551 |             29.3136 |\n",
            "\n",
            "==============================\n",
            "Processing Dataset: CICIDS2018_Reduced\n",
            "==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:40:04,523 - INFO - Dataset CICIDS2018_Reduced loaded. Shape: (2080710, 47)\n",
            "2025-04-12 23:40:04,524 - INFO - Starting sampling for CICIDS2018_Reduced...\n",
            "2025-04-12 23:40:04,524 - INFO - Starting imbalanced sampling. Target size for large classes: 50000\n",
            "2025-04-12 23:40:04,540 - INFO - Original class distribution:\\nLabel\n",
            "0    1497201\n",
            "1     583509\n",
            "Name: count, dtype: int64\n",
            "2025-04-12 23:40:04,541 - INFO - Found 2 large classes (>= 1000 samples).\n",
            "2025-04-12 23:40:04,542 - INFO - Scaling factor for large classes: 0.0240\n",
            "2025-04-12 23:40:04,542 - INFO -   Sampling class '0': target size=35978, final sample size=35978\n",
            "2025-04-12 23:40:04,965 - INFO -   Sampling class '1': target size=14021, final sample size=14021\n",
            "2025-04-12 23:40:05,193 - INFO - Finished sampling. New dataset size: 49999\n",
            "2025-04-12 23:40:05,195 - INFO - New class distribution:\\nLabel\n",
            "0    35978\n",
            "1    14021\n",
            "Name: count, dtype: int64\n",
            "2025-04-12 23:40:05,196 - INFO - Sampling complete for CICIDS2018_Reduced. Sampled shape: (49999, 47)\n",
            "2025-04-12 23:40:05,378 - INFO - Starting feature engineering for CICIDS2018_Reduced...\n",
            "2025-04-12 23:40:05,649 - INFO - Feature engineering complete for CICIDS2018_Reduced. X shape: (49999, 166), y shape: (49999,)\n",
            "2025-04-12 23:40:05,810 - INFO - Starting adaptive graph construction for CICIDS2018_Reduced...\n",
            "2025-04-12 23:40:05,812 - INFO - Starting adaptive graph construction for 49999 samples...\n",
            "2025-04-12 23:40:05,812 - INFO - Parameters: k=20, metric='euclidean', threshold=0.5\n",
            "2025-04-12 23:40:05,812 - INFO - Calculating k-NN graph...\n",
            "2025-04-12 23:40:08,739 - INFO - Calculated k-NN graph. Shape: (49999, 49999), NNZ: 999980. Time: 2.93s\n",
            "2025-04-12 23:40:08,741 - INFO - Calculating pairwise distances (using sklearn) and applying threshold...\n",
            "2025-04-12 23:40:25,659 - INFO - Calculated pairwise distances matrix. Shape: (49999, 49999). Time: 16.92s\n",
            "2025-04-12 23:40:27,390 - INFO - Created distance threshold adjacency mask.\n",
            "2025-04-12 23:40:47,755 - INFO - Converted distance mask to sparse CSR. NNZ: 170095434\n",
            "2025-04-12 23:40:48,018 - INFO - Intersecting k-NN (CSR) and distance threshold (CSR) graphs...\n",
            "2025-04-12 23:40:49,377 - INFO - Intersection complete. Final Adj NNZ: 955840. Time: 1.36s\n",
            "2025-04-12 23:40:49,393 - INFO - Final number of edges: 955840\n",
            "2025-04-12 23:40:49,399 - INFO - Created PyG Data object: Data(x=[49999, 166], edge_index=[2, 955840], y=[49999]). Time: 0.02s\n",
            "2025-04-12 23:40:49,586 - INFO - Cleaned up intermediate graph construction objects.\n",
            "2025-04-12 23:40:49,587 - INFO - Graph construction complete for CICIDS2018_Reduced. Data object: Data(x=[49999, 166], edge_index=[2, 955840], y=[49999])\n",
            "2025-04-12 23:40:49,741 - INFO - Creating train/val/test masks for CICIDS2018_Reduced...\n",
            "2025-04-12 23:40:49,759 - INFO - Masks created for CICIDS2018_Reduced. Train: 34999, Val: 7500, Test: 7500\n",
            "2025-04-12 23:40:49,940 - INFO - Moved data for CICIDS2018_Reduced to cuda\n",
            "2025-04-12 23:40:49,957 - INFO - Starting training for CAGN on CICIDS2018_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training CAGN for CICIDS2018_Reduced ---\n",
            "    Epoch: 001, Train Loss: 1.7133 (CLS: 1.2324, Contr: 0.9617), Val Loss: 0.8971 (CLS: 0.8971), Val F1: 0.6224, Val AUC: 0.8274\n",
            "    Epoch: 050, Train Loss: 0.9818 (CLS: 0.6230, Contr: 0.7176), Val Loss: 0.3747 (CLS: 0.3747), Val F1: 0.8583, Val AUC: 0.9562\n",
            "    Epoch: 100, Train Loss: 0.9382 (CLS: 0.5903, Contr: 0.6958), Val Loss: 0.3493 (CLS: 0.3493), Val F1: 0.8785, Val AUC: 0.9602\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:42:29,837 - INFO - Early stopping for CAGN on CICIDS2018_Reduced at epoch 149.\n",
            "2025-04-12 23:42:29,874 - INFO - Saved best CAGN model state for CICIDS2018_Reduced to /media/ssd/test/GNN/Standardized Models/CAGN-GAT/best_cagn_model_CICIDS2018_Reduced.pt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Test F1 for CAGN: 0.8761, AUC: 0.9623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:42:30,138 - INFO - Starting training for GCN on CICIDS2018_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training GCN for CICIDS2018_Reduced ---\n",
            "    Epoch: 001, Train Loss: 1.0860 (CLS: 1.0860), Val Loss: 1.0504 (CLS: 1.0504), Val F1: 0.2698, Val AUC: 0.3299\n",
            "    Epoch: 050, Train Loss: 0.4354 (CLS: 0.4354), Val Loss: 0.4197 (CLS: 0.4197), Val F1: 0.8453, Val AUC: 0.9481\n",
            "    Epoch: 100, Train Loss: 0.2793 (CLS: 0.2793), Val Loss: 0.2626 (CLS: 0.2626), Val F1: 0.9245, Val AUC: 0.9727\n",
            "    Epoch: 150, Train Loss: 0.2406 (CLS: 0.2406), Val Loss: 0.2309 (CLS: 0.2309), Val F1: 0.9431, Val AUC: 0.9759\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:42:35,246 - INFO - Starting training for BaselineGAT on CICIDS2018_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Epoch: 200, Train Loss: 0.2211 (CLS: 0.2211), Val Loss: 0.2167 (CLS: 0.2167), Val F1: 0.9470, Val AUC: 0.9778\n",
            "    Test F1 for GCN: 0.9455, AUC: 0.9787\n",
            "    --- Training BaselineGAT for CICIDS2018_Reduced ---\n",
            "    Epoch: 001, Train Loss: 1.0975 (CLS: 1.0975), Val Loss: 0.8761 (CLS: 0.8761), Val F1: 0.6312, Val AUC: 0.8441\n",
            "    Epoch: 050, Train Loss: 0.5565 (CLS: 0.5565), Val Loss: 0.3814 (CLS: 0.3814), Val F1: 0.8450, Val AUC: 0.9465\n",
            "    Epoch: 100, Train Loss: 0.4923 (CLS: 0.4923), Val Loss: 0.3163 (CLS: 0.3163), Val F1: 0.8615, Val AUC: 0.9635\n",
            "    Epoch: 150, Train Loss: 0.4684 (CLS: 0.4684), Val Loss: 0.2863 (CLS: 0.2863), Val F1: 0.8866, Val AUC: 0.9665\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:42:42,805 - INFO - Starting training for GraphSAGE on CICIDS2018_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Epoch: 200, Train Loss: 0.4508 (CLS: 0.4508), Val Loss: 0.2725 (CLS: 0.2725), Val F1: 0.9128, Val AUC: 0.9668\n",
            "    Test F1 for BaselineGAT: 0.9081, AUC: 0.9678\n",
            "    --- Training GraphSAGE for CICIDS2018_Reduced ---\n",
            "    Epoch: 001, Train Loss: 0.9992 (CLS: 0.9992), Val Loss: 0.9851 (CLS: 0.9851), Val F1: 0.5148, Val AUC: 0.7066\n",
            "    Epoch: 050, Train Loss: 0.4006 (CLS: 0.4006), Val Loss: 0.3786 (CLS: 0.3786), Val F1: 0.8741, Val AUC: 0.9561\n",
            "    Epoch: 100, Train Loss: 0.2630 (CLS: 0.2630), Val Loss: 0.2531 (CLS: 0.2531), Val F1: 0.9278, Val AUC: 0.9708\n",
            "    Epoch: 150, Train Loss: 0.2267 (CLS: 0.2267), Val Loss: 0.2198 (CLS: 0.2198), Val F1: 0.9445, Val AUC: 0.9772\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:42:49,064 - INFO - Starting training for GIN on CICIDS2018_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Epoch: 200, Train Loss: 0.2124 (CLS: 0.2124), Val Loss: 0.2068 (CLS: 0.2068), Val F1: 0.9434, Val AUC: 0.9788\n",
            "    Test F1 for GraphSAGE: 0.9451, AUC: 0.9772\n",
            "    --- Training GIN for CICIDS2018_Reduced ---\n",
            "    Epoch: 001, Train Loss: 1.1166 (CLS: 1.1166), Val Loss: 0.9420 (CLS: 0.9420), Val F1: 0.4982, Val AUC: 0.7778\n",
            "    Epoch: 050, Train Loss: 0.4640 (CLS: 0.4640), Val Loss: 0.4822 (CLS: 0.4822), Val F1: 0.7852, Val AUC: 0.9435\n",
            "    Epoch: 100, Train Loss: 0.2262 (CLS: 0.2262), Val Loss: 0.3945 (CLS: 0.3945), Val F1: 0.7914, Val AUC: 0.9691\n",
            "    Epoch: 150, Train Loss: 0.1729 (CLS: 0.1729), Val Loss: 0.2076 (CLS: 0.2076), Val F1: 0.9548, Val AUC: 0.9791\n",
            "    Epoch: 200, Train Loss: 0.1672 (CLS: 0.1672), Val Loss: 0.1857 (CLS: 0.1857), Val F1: 0.9525, Val AUC: 0.9827\n",
            "    Test F1 for GIN: 0.9544, AUC: 0.9837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:42:56,216 - INFO - Starting processing for dataset: BotIoT_v2_Reduced from /media/ssd/test/standardized-datasets/netflow/nf_bot_iot_v2_reduced_standardized.csv\n",
            "2025-04-12 23:42:56,217 - INFO - Loading dataset: BotIoT_v2_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Results for CICIDS2018_Reduced ---\n",
            "| Model       |   Accuracy |   Precision |   Recall |     F1 |    AUC |   Test Loss (CLS) |   Training Time (s) |\n",
            "|:------------|-----------:|------------:|---------:|-------:|-------:|------------------:|--------------------:|\n",
            "| GIN         |     0.9752 |      0.9853 |   0.9253 | 0.9544 | 0.9837 |            0.1803 |              6.7868 |\n",
            "| GCN         |     0.9703 |      0.9728 |   0.9196 | 0.9455 | 0.9787 |            0.2152 |              4.9405 |\n",
            "| GraphSAGE   |     0.97   |      0.9704 |   0.9211 | 0.9451 | 0.9772 |            0.2094 |              6.0875 |\n",
            "| BaselineGAT |     0.9481 |      0.9027 |   0.9135 | 0.9081 | 0.9678 |            0.2715 |              7.3861 |\n",
            "| CAGN        |     0.9287 |      0.8541 |   0.8992 | 0.8761 | 0.9623 |            0.3424 |             99.8804 |\n",
            "\n",
            "==============================\n",
            "Processing Dataset: BotIoT_v2_Reduced\n",
            "==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:42:57,994 - INFO - Dataset BotIoT_v2_Reduced loaded. Shape: (345256, 47)\n",
            "2025-04-12 23:42:57,996 - INFO - Starting sampling for BotIoT_v2_Reduced...\n",
            "2025-04-12 23:42:57,996 - INFO - Starting imbalanced sampling. Target size for large classes: 50000\n",
            "2025-04-12 23:42:58,000 - INFO - Original class distribution:\\nLabel\n",
            "1    339853\n",
            "0      5403\n",
            "Name: count, dtype: int64\n",
            "2025-04-12 23:42:58,001 - INFO - Found 2 large classes (>= 1000 samples).\n",
            "2025-04-12 23:42:58,001 - INFO - Scaling factor for large classes: 0.1448\n",
            "2025-04-12 23:42:58,002 - INFO -   Sampling class '1': target size=49217, final sample size=49217\n",
            "2025-04-12 23:42:58,071 - INFO -   Sampling class '0': target size=782, final sample size=782\n",
            "2025-04-12 23:42:58,103 - INFO - Finished sampling. New dataset size: 49999\n",
            "2025-04-12 23:42:58,105 - INFO - New class distribution:\\nLabel\n",
            "1    49217\n",
            "0      782\n",
            "Name: count, dtype: int64\n",
            "2025-04-12 23:42:58,105 - INFO - Sampling complete for BotIoT_v2_Reduced. Sampled shape: (49999, 47)\n",
            "2025-04-12 23:42:58,268 - INFO - Starting feature engineering for BotIoT_v2_Reduced...\n",
            "2025-04-12 23:42:58,429 - INFO - Feature engineering complete for BotIoT_v2_Reduced. X shape: (49999, 68), y shape: (49999,)\n",
            "2025-04-12 23:42:58,592 - INFO - Starting adaptive graph construction for BotIoT_v2_Reduced...\n",
            "2025-04-12 23:42:58,593 - INFO - Starting adaptive graph construction for 49999 samples...\n",
            "2025-04-12 23:42:58,594 - INFO - Parameters: k=20, metric='euclidean', threshold=0.5\n",
            "2025-04-12 23:42:58,594 - INFO - Calculating k-NN graph...\n",
            "2025-04-12 23:43:00,227 - INFO - Calculated k-NN graph. Shape: (49999, 49999), NNZ: 999980. Time: 1.63s\n",
            "2025-04-12 23:43:00,228 - INFO - Calculating pairwise distances (using sklearn) and applying threshold...\n",
            "2025-04-12 23:43:16,460 - INFO - Calculated pairwise distances matrix. Shape: (49999, 49999). Time: 16.23s\n",
            "2025-04-12 23:43:18,648 - INFO - Created distance threshold adjacency mask.\n",
            "2025-04-12 23:43:58,209 - INFO - Converted distance mask to sparse CSR. NNZ: 585936554\n",
            "2025-04-12 23:43:58,445 - INFO - Intersecting k-NN (CSR) and distance threshold (CSR) graphs...\n",
            "2025-04-12 23:44:02,725 - INFO - Intersection complete. Final Adj NNZ: 992376. Time: 4.28s\n",
            "2025-04-12 23:44:02,741 - INFO - Final number of edges: 992376\n",
            "2025-04-12 23:44:02,748 - INFO - Created PyG Data object: Data(x=[49999, 68], edge_index=[2, 992376], y=[49999]). Time: 0.02s\n",
            "2025-04-12 23:44:02,908 - INFO - Cleaned up intermediate graph construction objects.\n",
            "2025-04-12 23:44:02,909 - INFO - Graph construction complete for BotIoT_v2_Reduced. Data object: Data(x=[49999, 68], edge_index=[2, 992376], y=[49999])\n",
            "2025-04-12 23:44:03,058 - INFO - Creating train/val/test masks for BotIoT_v2_Reduced...\n",
            "2025-04-12 23:44:03,076 - INFO - Masks created for BotIoT_v2_Reduced. Train: 34999, Val: 7500, Test: 7500\n",
            "2025-04-12 23:44:03,256 - INFO - Moved data for BotIoT_v2_Reduced to cuda\n",
            "2025-04-12 23:44:03,271 - INFO - Starting training for CAGN on BotIoT_v2_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training CAGN for BotIoT_v2_Reduced ---\n",
            "    Epoch: 001, Train Loss: 0.5182 (CLS: 0.0296, Contr: 0.9772), Val Loss: 0.0176 (CLS: 0.0176), Val F1: 0.9242, Val AUC: 0.9179\n",
            "    Epoch: 050, Train Loss: 0.3415 (CLS: 0.0169, Contr: 0.6492), Val Loss: 0.0100 (CLS: 0.0100), Val F1: 0.9670, Val AUC: 0.9226\n",
            "    Epoch: 100, Train Loss: 0.3358 (CLS: 0.0170, Contr: 0.6378), Val Loss: 0.0095 (CLS: 0.0095), Val F1: 0.9682, Val AUC: 0.9392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:45:18,875 - INFO - Early stopping for CAGN on BotIoT_v2_Reduced at epoch 112.\n",
            "2025-04-12 23:45:18,912 - INFO - Saved best CAGN model state for BotIoT_v2_Reduced to /media/ssd/test/GNN/Standardized Models/CAGN-GAT/best_cagn_model_BotIoT_v2_Reduced.pt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Test F1 for CAGN: 0.9688, AUC: 0.9638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:45:19,180 - INFO - Starting training for GCN on BotIoT_v2_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training GCN for BotIoT_v2_Reduced ---\n",
            "    Epoch: 001, Train Loss: 0.0244 (CLS: 0.0244), Val Loss: 0.0217 (CLS: 0.0217), Val F1: 0.0000, Val AUC: 0.7933\n",
            "    Epoch: 050, Train Loss: 0.0103 (CLS: 0.0103), Val Loss: 0.0093 (CLS: 0.0093), Val F1: 0.9242, Val AUC: 0.9796\n",
            "    Epoch: 100, Train Loss: 0.0065 (CLS: 0.0065), Val Loss: 0.0058 (CLS: 0.0058), Val F1: 0.9679, Val AUC: 0.9879\n",
            "    Epoch: 150, Train Loss: 0.0051 (CLS: 0.0051), Val Loss: 0.0047 (CLS: 0.0047), Val F1: 0.9765, Val AUC: 0.9913\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:45:24,216 - INFO - Starting training for BaselineGAT on BotIoT_v2_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Epoch: 200, Train Loss: 0.0044 (CLS: 0.0044), Val Loss: 0.0042 (CLS: 0.0042), Val F1: 0.9766, Val AUC: 0.9935\n",
            "    Test F1 for GCN: 0.9779, AUC: 0.9939\n",
            "    --- Training BaselineGAT for BotIoT_v2_Reduced ---\n",
            "    Epoch: 001, Train Loss: 0.0480 (CLS: 0.0480), Val Loss: 0.0233 (CLS: 0.0233), Val F1: 0.0519, Val AUC: 0.4818\n",
            "    Epoch: 050, Train Loss: 0.0171 (CLS: 0.0171), Val Loss: 0.0096 (CLS: 0.0096), Val F1: 0.9241, Val AUC: 0.9577\n",
            "    Epoch: 100, Train Loss: 0.0130 (CLS: 0.0130), Val Loss: 0.0081 (CLS: 0.0081), Val F1: 0.9243, Val AUC: 0.9586\n",
            "    Epoch: 150, Train Loss: 0.0133 (CLS: 0.0133), Val Loss: 0.0074 (CLS: 0.0074), Val F1: 0.9243, Val AUC: 0.9607\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:45:31,791 - INFO - Starting training for GraphSAGE on BotIoT_v2_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Epoch: 200, Train Loss: 0.0113 (CLS: 0.0113), Val Loss: 0.0070 (CLS: 0.0070), Val F1: 0.9243, Val AUC: 0.9637\n",
            "    Test F1 for BaselineGAT: 0.9245, AUC: 0.9680\n",
            "    --- Training GraphSAGE for BotIoT_v2_Reduced ---\n",
            "    Epoch: 001, Train Loss: 0.0212 (CLS: 0.0212), Val Loss: 0.0205 (CLS: 0.0205), Val F1: 0.1129, Val AUC: 0.9555\n",
            "    Epoch: 050, Train Loss: 0.0089 (CLS: 0.0089), Val Loss: 0.0085 (CLS: 0.0085), Val F1: 0.9499, Val AUC: 0.9879\n",
            "    Epoch: 100, Train Loss: 0.0045 (CLS: 0.0045), Val Loss: 0.0042 (CLS: 0.0042), Val F1: 0.9691, Val AUC: 0.9914\n",
            "    Epoch: 150, Train Loss: 0.0038 (CLS: 0.0038), Val Loss: 0.0037 (CLS: 0.0037), Val F1: 0.9691, Val AUC: 0.9919\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:45:36,456 - INFO - Starting training for GIN on BotIoT_v2_Reduced\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Epoch: 200, Train Loss: 0.0036 (CLS: 0.0036), Val Loss: 0.0035 (CLS: 0.0035), Val F1: 0.9731, Val AUC: 0.9927\n",
            "    Test F1 for GraphSAGE: 0.9737, AUC: 0.9941\n",
            "    --- Training GIN for BotIoT_v2_Reduced ---\n",
            "    Epoch: 001, Train Loss: 0.0242 (CLS: 0.0242), Val Loss: 0.0457 (CLS: 0.0457), Val F1: 0.0000, Val AUC: 0.5236\n",
            "    Epoch: 050, Train Loss: 0.0112 (CLS: 0.0112), Val Loss: 0.0139 (CLS: 0.0139), Val F1: 0.9900, Val AUC: 0.9252\n",
            "    Epoch: 100, Train Loss: 0.0068 (CLS: 0.0068), Val Loss: 0.0096 (CLS: 0.0096), Val F1: 0.9725, Val AUC: 0.9660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:45:40,199 - INFO - Early stopping for GIN on BotIoT_v2_Reduced at epoch 142.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Test F1 for GIN: 0.9777, AUC: 0.9664\n",
            "\n",
            "--- Results for BotIoT_v2_Reduced ---\n",
            "| Model       |   Accuracy |   Precision |   Recall |     F1 |    AUC |   Test Loss (CLS) |   Training Time (s) |\n",
            "|:------------|-----------:|------------:|---------:|-------:|-------:|------------------:|--------------------:|\n",
            "| GCN         |     0.9573 |      0.9997 |   0.9569 | 0.9779 | 0.9939 |            0.004  |              4.8673 |\n",
            "| GIN         |     0.9569 |      0.9992 |   0.9571 | 0.9777 | 0.9664 |            0.0078 |              3.7422 |\n",
            "| GraphSAGE   |     0.9496 |      0.9999 |   0.9489 | 0.9737 | 0.9941 |            0.0035 |              4.497  |\n",
            "| CAGN        |     0.9404 |      0.9993 |   0.9401 | 0.9688 | 0.9638 |            0.0086 |             75.6037 |\n",
            "| BaselineGAT |     0.8617 |      0.9998 |   0.8597 | 0.9245 | 0.968  |            0.0068 |              7.4019 |\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:45:40,533 - INFO - Starting processing for dataset: UNSW_NB15 from /media/ssd/test/standardized-datasets/netflow/nf_unsw_nb15_standardized.csv\n",
            "2025-04-12 23:45:40,533 - INFO - Loading dataset: UNSW_NB15\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================\n",
            "Processing Dataset: UNSW_NB15\n",
            "==============================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:45:52,967 - INFO - Dataset UNSW_NB15 loaded. Shape: (2390275, 47)\n",
            "2025-04-12 23:45:52,969 - INFO - Starting sampling for UNSW_NB15...\n",
            "2025-04-12 23:45:52,969 - INFO - Starting imbalanced sampling. Target size for large classes: 50000\n",
            "2025-04-12 23:45:52,987 - INFO - Original class distribution:\\nLabel\n",
            "0    2295222\n",
            "1      95053\n",
            "Name: count, dtype: int64\n",
            "2025-04-12 23:45:52,989 - INFO - Found 2 large classes (>= 1000 samples).\n",
            "2025-04-12 23:45:52,989 - INFO - Scaling factor for large classes: 0.0209\n",
            "2025-04-12 23:45:52,990 - INFO -   Sampling class '0': target size=48011, final sample size=48011\n",
            "2025-04-12 23:45:53,593 - INFO -   Sampling class '1': target size=1988, final sample size=1988\n",
            "2025-04-12 23:45:53,673 - INFO - Finished sampling. New dataset size: 49999\n",
            "2025-04-12 23:45:53,675 - INFO - New class distribution:\\nLabel\n",
            "0    48011\n",
            "1     1988\n",
            "Name: count, dtype: int64\n",
            "2025-04-12 23:45:53,676 - INFO - Sampling complete for UNSW_NB15. Sampled shape: (49999, 47)\n",
            "2025-04-12 23:45:53,893 - INFO - Starting feature engineering for UNSW_NB15...\n",
            "2025-04-12 23:45:54,255 - INFO - Feature engineering complete for UNSW_NB15. X shape: (49999, 222), y shape: (49999,)\n",
            "2025-04-12 23:45:54,419 - INFO - Starting adaptive graph construction for UNSW_NB15...\n",
            "2025-04-12 23:45:54,420 - INFO - Starting adaptive graph construction for 49999 samples...\n",
            "2025-04-12 23:45:54,420 - INFO - Parameters: k=20, metric='euclidean', threshold=0.5\n",
            "2025-04-12 23:45:54,421 - INFO - Calculating k-NN graph...\n",
            "2025-04-12 23:45:58,051 - INFO - Calculated k-NN graph. Shape: (49999, 49999), NNZ: 999980. Time: 3.63s\n",
            "2025-04-12 23:45:58,052 - INFO - Calculating pairwise distances (using sklearn) and applying threshold...\n",
            "2025-04-12 23:46:15,860 - INFO - Calculated pairwise distances matrix. Shape: (49999, 49999). Time: 17.81s\n",
            "2025-04-12 23:46:17,571 - INFO - Created distance threshold adjacency mask.\n",
            "2025-04-12 23:46:38,138 - INFO - Converted distance mask to sparse CSR. NNZ: 173351346\n",
            "2025-04-12 23:46:38,377 - INFO - Intersecting k-NN (CSR) and distance threshold (CSR) graphs...\n",
            "2025-04-12 23:46:39,963 - INFO - Intersection complete. Final Adj NNZ: 979819. Time: 1.59s\n",
            "2025-04-12 23:46:39,982 - INFO - Final number of edges: 979819\n",
            "2025-04-12 23:46:39,990 - INFO - Created PyG Data object: Data(x=[49999, 222], edge_index=[2, 979819], y=[49999]). Time: 0.03s\n",
            "2025-04-12 23:46:40,190 - INFO - Cleaned up intermediate graph construction objects.\n",
            "2025-04-12 23:46:40,190 - INFO - Graph construction complete for UNSW_NB15. Data object: Data(x=[49999, 222], edge_index=[2, 979819], y=[49999])\n",
            "2025-04-12 23:46:40,341 - INFO - Creating train/val/test masks for UNSW_NB15...\n",
            "2025-04-12 23:46:40,359 - INFO - Masks created for UNSW_NB15. Train: 34999, Val: 7500, Test: 7500\n",
            "2025-04-12 23:46:40,543 - INFO - Moved data for UNSW_NB15 to cuda\n",
            "2025-04-12 23:46:40,561 - INFO - Starting training for CAGN on UNSW_NB15\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training CAGN for UNSW_NB15 ---\n",
            "    Epoch: 001, Train Loss: 2.4032 (CLS: 1.9140, Contr: 0.9783), Val Loss: 1.2883 (CLS: 1.2883), Val F1: 0.1175, Val AUC: 0.6244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:47:07,698 - INFO - Early stopping for CAGN on UNSW_NB15 at epoch 40.\n",
            "2025-04-12 23:47:07,735 - INFO - Saved best CAGN model state for UNSW_NB15 to /media/ssd/test/GNN/Standardized Models/CAGN-GAT/best_cagn_model_UNSW_NB15.pt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Test F1 for CAGN: 0.1824, AUC: 0.8942\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:47:08,000 - INFO - Starting training for GCN on UNSW_NB15\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training GCN for UNSW_NB15 ---\n",
            "    Epoch: 001, Train Loss: 1.3907 (CLS: 1.3907), Val Loss: 1.3303 (CLS: 1.3303), Val F1: 0.0807, Val AUC: 0.5779\n",
            "    Epoch: 050, Train Loss: 0.7065 (CLS: 0.7065), Val Loss: 0.6553 (CLS: 0.6553), Val F1: 0.4467, Val AUC: 0.9370\n",
            "    Epoch: 100, Train Loss: 0.4936 (CLS: 0.4936), Val Loss: 0.4758 (CLS: 0.4758), Val F1: 0.5074, Val AUC: 0.9671\n",
            "    Epoch: 150, Train Loss: 0.4191 (CLS: 0.4191), Val Loss: 0.4191 (CLS: 0.4191), Val F1: 0.6033, Val AUC: 0.9772\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:47:13,092 - INFO - Starting training for BaselineGAT on UNSW_NB15\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Epoch: 200, Train Loss: 0.3578 (CLS: 0.3578), Val Loss: 0.3768 (CLS: 0.3768), Val F1: 0.4179, Val AUC: 0.9820\n",
            "    Test F1 for GCN: 0.4212, AUC: 0.9864\n",
            "    --- Training BaselineGAT for UNSW_NB15 ---\n",
            "    Epoch: 001, Train Loss: 1.7527 (CLS: 1.7527), Val Loss: 1.3513 (CLS: 1.3513), Val F1: 0.0706, Val AUC: 0.4891\n",
            "    Epoch: 050, Train Loss: 0.9758 (CLS: 0.9758), Val Loss: 0.7231 (CLS: 0.7231), Val F1: 0.4066, Val AUC: 0.9342\n",
            "    Epoch: 100, Train Loss: 0.8462 (CLS: 0.8462), Val Loss: 0.5814 (CLS: 0.5814), Val F1: 0.3631, Val AUC: 0.9522\n",
            "    Epoch: 150, Train Loss: 0.8078 (CLS: 0.8078), Val Loss: 0.5622 (CLS: 0.5622), Val F1: 0.3703, Val AUC: 0.9531\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:47:20,761 - INFO - Starting training for GraphSAGE on UNSW_NB15\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Epoch: 200, Train Loss: 0.7819 (CLS: 0.7819), Val Loss: 0.5558 (CLS: 0.5558), Val F1: 0.3696, Val AUC: 0.9534\n",
            "    Test F1 for BaselineGAT: 0.3829, AUC: 0.9522\n",
            "    --- Training GraphSAGE for UNSW_NB15 ---\n",
            "    Epoch: 001, Train Loss: 1.3195 (CLS: 1.3195), Val Loss: 1.3116 (CLS: 1.3116), Val F1: 0.0764, Val AUC: 0.8215\n",
            "    Epoch: 050, Train Loss: 0.6804 (CLS: 0.6804), Val Loss: 0.6695 (CLS: 0.6695), Val F1: 0.5332, Val AUC: 0.9536\n",
            "    Epoch: 100, Train Loss: 0.2843 (CLS: 0.2843), Val Loss: 0.2693 (CLS: 0.2693), Val F1: 0.7530, Val AUC: 0.9837\n",
            "    Epoch: 150, Train Loss: 0.2071 (CLS: 0.2071), Val Loss: 0.1858 (CLS: 0.1858), Val F1: 0.7872, Val AUC: 0.9947\n",
            "    Epoch: 200, Train Loss: 0.1688 (CLS: 0.1688), Val Loss: 0.1496 (CLS: 0.1496), Val F1: 0.8051, Val AUC: 0.9964\n",
            "    Test F1 for GraphSAGE: 0.8326, AUC: 0.9958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-12 23:47:28,102 - INFO - Starting training for GIN on UNSW_NB15\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    --- Training GIN for UNSW_NB15 ---\n",
            "    Epoch: 001, Train Loss: 1.3982 (CLS: 1.3982), Val Loss: 1.4246 (CLS: 1.4246), Val F1: 0.0762, Val AUC: 0.4905\n",
            "    Epoch: 050, Train Loss: 0.7010 (CLS: 0.7010), Val Loss: 1.0064 (CLS: 1.0064), Val F1: 0.6201, Val AUC: 0.9435\n",
            "    Epoch: 100, Train Loss: 0.3431 (CLS: 0.3431), Val Loss: 0.4849 (CLS: 0.4849), Val F1: 0.8646, Val AUC: 0.9939\n",
            "    Epoch: 150, Train Loss: 0.1620 (CLS: 0.1620), Val Loss: 0.1775 (CLS: 0.1775), Val F1: 0.8943, Val AUC: 0.9977\n",
            "    Epoch: 200, Train Loss: 0.2134 (CLS: 0.2134), Val Loss: 0.9718 (CLS: 0.9718), Val F1: 0.3267, Val AUC: 0.9859\n",
            "    Test F1 for GIN: 0.8702, AUC: 0.9973\n",
            "\n",
            "--- Results for UNSW_NB15 ---\n",
            "| Model       |   Accuracy |   Precision |   Recall |     F1 |    AUC |   Test Loss (CLS) |   Training Time (s) |\n",
            "|:------------|-----------:|------------:|---------:|-------:|-------:|------------------:|--------------------:|\n",
            "| GIN         |     0.9883 |      0.7763 |   0.9899 | 0.8702 | 0.9973 |            0.1336 |              7.8923 |\n",
            "| GraphSAGE   |     0.9844 |      0.7257 |   0.9765 | 0.8326 | 0.9958 |            0.134  |              7.1234 |\n",
            "| GCN         |     0.8967 |      0.2709 |   0.9463 | 0.4212 | 0.9864 |            0.3402 |              4.9212 |\n",
            "| BaselineGAT |     0.8883 |      0.2453 |   0.8725 | 0.3829 | 0.9522 |            0.5507 |              7.4838 |\n",
            "| CAGN        |     0.6964 |      0.1021 |   0.8523 | 0.1824 | 0.8942 |            0.882  |             27.1372 |\n",
            "\n",
            "==============================\n",
            "Benchmark Complete\n",
            "==============================\n",
            "\n",
            "--- Overall Benchmark Summary ---\n",
            "\n",
            "--- Results for Combined_10pct ---\n",
            "| Model       |   Accuracy |   Precision |   Recall |     F1 |    AUC |   Test Loss (CLS) |   Training Time (s) |\n",
            "|:------------|-----------:|------------:|---------:|-------:|-------:|------------------:|--------------------:|\n",
            "| GIN         |     0.9669 |      0.976  |   0.9543 | 0.965  | 0.9933 |            0.1036 |             10.5824 |\n",
            "| GraphSAGE   |     0.9579 |      0.9669 |   0.9442 | 0.9554 | 0.9855 |            0.1432 |              9.9704 |\n",
            "| GCN         |     0.9469 |      0.9645 |   0.9231 | 0.9433 | 0.977  |            0.186  |              4.8885 |\n",
            "| BaselineGAT |     0.9321 |      0.9296 |   0.9284 | 0.929  | 0.9725 |            0.2105 |              7.3028 |\n",
            "| CAGN        |     0.8507 |      0.9484 |   0.7273 | 0.8233 | 0.9582 |            0.3551 |             29.3136 |\n",
            "\n",
            "--- Results for CICIDS2018_Reduced ---\n",
            "| Model       |   Accuracy |   Precision |   Recall |     F1 |    AUC |   Test Loss (CLS) |   Training Time (s) |\n",
            "|:------------|-----------:|------------:|---------:|-------:|-------:|------------------:|--------------------:|\n",
            "| GIN         |     0.9752 |      0.9853 |   0.9253 | 0.9544 | 0.9837 |            0.1803 |              6.7868 |\n",
            "| GCN         |     0.9703 |      0.9728 |   0.9196 | 0.9455 | 0.9787 |            0.2152 |              4.9405 |\n",
            "| GraphSAGE   |     0.97   |      0.9704 |   0.9211 | 0.9451 | 0.9772 |            0.2094 |              6.0875 |\n",
            "| BaselineGAT |     0.9481 |      0.9027 |   0.9135 | 0.9081 | 0.9678 |            0.2715 |              7.3861 |\n",
            "| CAGN        |     0.9287 |      0.8541 |   0.8992 | 0.8761 | 0.9623 |            0.3424 |             99.8804 |\n",
            "\n",
            "--- Results for BotIoT_v2_Reduced ---\n",
            "| Model       |   Accuracy |   Precision |   Recall |     F1 |    AUC |   Test Loss (CLS) |   Training Time (s) |\n",
            "|:------------|-----------:|------------:|---------:|-------:|-------:|------------------:|--------------------:|\n",
            "| GCN         |     0.9573 |      0.9997 |   0.9569 | 0.9779 | 0.9939 |            0.004  |              4.8673 |\n",
            "| GIN         |     0.9569 |      0.9992 |   0.9571 | 0.9777 | 0.9664 |            0.0078 |              3.7422 |\n",
            "| GraphSAGE   |     0.9496 |      0.9999 |   0.9489 | 0.9737 | 0.9941 |            0.0035 |              4.497  |\n",
            "| CAGN        |     0.9404 |      0.9993 |   0.9401 | 0.9688 | 0.9638 |            0.0086 |             75.6037 |\n",
            "| BaselineGAT |     0.8617 |      0.9998 |   0.8597 | 0.9245 | 0.968  |            0.0068 |              7.4019 |\n",
            "\n",
            "--- Results for UNSW_NB15 ---\n",
            "| Model       |   Accuracy |   Precision |   Recall |     F1 |    AUC |   Test Loss (CLS) |   Training Time (s) |\n",
            "|:------------|-----------:|------------:|---------:|-------:|-------:|------------------:|--------------------:|\n",
            "| GIN         |     0.9883 |      0.7763 |   0.9899 | 0.8702 | 0.9973 |            0.1336 |              7.8923 |\n",
            "| GraphSAGE   |     0.9844 |      0.7257 |   0.9765 | 0.8326 | 0.9958 |            0.134  |              7.1234 |\n",
            "| GCN         |     0.8967 |      0.2709 |   0.9463 | 0.4212 | 0.9864 |            0.3402 |              4.9212 |\n",
            "| BaselineGAT |     0.8883 |      0.2453 |   0.8725 | 0.3829 | 0.9522 |            0.5507 |              7.4838 |\n",
            "| CAGN        |     0.6964 |      0.1021 |   0.8523 | 0.1824 | 0.8942 |            0.882  |             27.1372 |\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Main Dataset Benchmarking Loop\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import gc\n",
        "import logging\n",
        "import time\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from torch_geometric.data import Data\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "from scipy import sparse\n",
        "import torch.optim as optim # Make sure optim is imported here if not globally\n",
        "import torch.nn as nn      # Make sure nn is imported here if not globally\n",
        "\n",
        "# --- Dataset Definitions ---\n",
        "datasets_to_benchmark = {\n",
        "    \"Combined_10pct\": \"/media/ssd/test/standardized-datasets/combined/combined_unsw_cicRed_botRed_netflow_10pct.csv\",\n",
        "    \"CICIDS2018_Reduced\": \"/media/ssd/test/standardized-datasets/netflow/nf_cic_ids2018_reduced_standardized.csv\",\n",
        "    \"BotIoT_v2_Reduced\": \"/media/ssd/test/standardized-datasets/netflow/nf_bot_iot_v2_reduced_standardized.csv\",\n",
        "    \"UNSW_NB15\": \"/media/ssd/test/standardized-datasets/netflow/nf_unsw_nb15_standardized.csv\"\n",
        "}\n",
        "\n",
        "# --- Configurations (Defined within this cell for clarity) ---\n",
        "# Make sure REQUIRED_COLUMNS, numerical_cols, categorical_cols are defined globally or adjusted here if needed per dataset\n",
        "label_col = 'Label' # Assuming 'Label' is the target column in all standardized datasets\n",
        "SAMPLED_SIZE_LARGE_CLASSES = 50000 # Or adjust as needed\n",
        "MIN_LARGE_CLASS_SIZE = 1000\n",
        "learning_rate = 0.001\n",
        "weight_decay = 5e-4\n",
        "epochs = 200\n",
        "patience = 20\n",
        "hidden_dim = 64       # Shared hidden dimension\n",
        "gat_heads = 8         # Heads for GAT/CAGN\n",
        "dropout_rate = 0.6    # Dropout for GAT/CAGN (<<<< DEFINED HERE)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_dataset_results = {} # Store results for each dataset\n",
        "\n",
        "# --- Loop Through Datasets ---\n",
        "for dataset_name, dataset_path in datasets_to_benchmark.items():\n",
        "    print(f\"\\n{'='*30}\\nProcessing Dataset: {dataset_name}\\n{'='*30}\")\n",
        "    logging.info(f\"Starting processing for dataset: {dataset_name} from {dataset_path}\")\n",
        "\n",
        "    # --- 1. Data Loading ---\n",
        "    try:\n",
        "        logging.info(f\"Loading dataset: {dataset_name}\")\n",
        "        df = pd.read_csv(dataset_path, low_memory=False)\n",
        "        logging.info(f\"Dataset {dataset_name} loaded. Shape: {df.shape}\")\n",
        "        # Basic verification\n",
        "        if label_col not in df.columns:\n",
        "            logging.error(f\"Dataset {dataset_name} missing target column: '{label_col}'\")\n",
        "            continue\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Dataset file not found: {dataset_path}\")\n",
        "        continue\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading dataset {dataset_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # --- 2. Sampling ---\n",
        "    logging.info(f\"Starting sampling for {dataset_name}...\")\n",
        "    # Make sure create_imbalanced_subset function is defined globally (e.g., from original Cell 1.5)\n",
        "    try:\n",
        "        df_sampled = create_imbalanced_subset(\n",
        "            df,\n",
        "            target_col=label_col,\n",
        "            new_dataset_size_large_classes=SAMPLED_SIZE_LARGE_CLASSES,\n",
        "            min_large_class_size=MIN_LARGE_CLASS_SIZE\n",
        "        )\n",
        "        if df_sampled is None or df_sampled.empty:\n",
        "             logging.warning(f\"Sampling resulted in empty dataframe for {dataset_name}. Skipping.\")\n",
        "             del df\n",
        "             gc.collect()\n",
        "             continue\n",
        "        logging.info(f\"Sampling complete for {dataset_name}. Sampled shape: {df_sampled.shape}\")\n",
        "        del df\n",
        "        gc.collect()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during sampling for {dataset_name}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # --- 3. Feature Engineering ---\n",
        "    logging.info(f\"Starting feature engineering for {dataset_name}...\")\n",
        "    try:\n",
        "        # Ensure numerical_cols and categorical_cols are defined globally or adjusted if they vary per dataset\n",
        "        numerical_cols = ['IN_BYTES', 'OUT_BYTES', 'IN_PKTS', 'OUT_PKTS', 'FLOW_DURATION_MILLISECONDS'] # Example\n",
        "        categorical_cols = ['PROTOCOL', 'L7_PROTO', 'TCP_FLAGS'] # Example\n",
        "\n",
        "        columns_to_process = numerical_cols + categorical_cols + [label_col]\n",
        "        missing_processing_cols = [col for col in columns_to_process if col not in df_sampled.columns]\n",
        "        if missing_processing_cols:\n",
        "             logging.error(f\"Sampled {dataset_name} missing columns for FE: {missing_processing_cols}\")\n",
        "             del df_sampled\n",
        "             gc.collect()\n",
        "             continue\n",
        "\n",
        "        flows_df_processed = df_sampled[columns_to_process].copy()\n",
        "\n",
        "        # --- Process Numerical ---\n",
        "        for col in numerical_cols:\n",
        "            flows_df_processed[col] = pd.to_numeric(flows_df_processed[col], errors='coerce')\n",
        "        flows_df_processed[numerical_cols] = flows_df_processed[numerical_cols].fillna(0)\n",
        "        log_transformed_features = np.log1p(flows_df_processed[numerical_cols].values)\n",
        "        scaler = StandardScaler()\n",
        "        scaled_numerical_features = scaler.fit_transform(log_transformed_features)\n",
        "        scaled_numerical_df = pd.DataFrame(scaled_numerical_features, index=flows_df_processed.index, columns=numerical_cols)\n",
        "\n",
        "        # --- Process Categorical ---\n",
        "        flows_df_processed[categorical_cols] = flows_df_processed[categorical_cols].astype(str).fillna('missing')\n",
        "        categorical_encoded_df = pd.get_dummies(\n",
        "            flows_df_processed[categorical_cols],\n",
        "            columns=categorical_cols,\n",
        "            prefix=categorical_cols,\n",
        "            dummy_na=False,\n",
        "            dtype=int\n",
        "        )\n",
        "\n",
        "        # --- Combine Features ---\n",
        "        X_df = pd.concat([scaled_numerical_df, categorical_encoded_df.set_index(scaled_numerical_df.index)], axis=1)\n",
        "        X = X_df.values\n",
        "        y = flows_df_processed[label_col].values.astype(np.int64)\n",
        "\n",
        "        logging.info(f\"Feature engineering complete for {dataset_name}. X shape: {X.shape}, y shape: {y.shape}\")\n",
        "        del df_sampled, flows_df_processed, log_transformed_features, scaled_numerical_features\n",
        "        del scaled_numerical_df, categorical_encoded_df, X_df\n",
        "        gc.collect()\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during feature engineering for {dataset_name}: {e}\")\n",
        "        if 'df_sampled' in locals(): del df_sampled\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    # --- 4. Graph Construction ---\n",
        "    logging.info(f\"Starting adaptive graph construction for {dataset_name}...\")\n",
        "    # Make sure adaptive_graph_construction function is defined globally (e.g., from original Cell 3)\n",
        "    try:\n",
        "        data = adaptive_graph_construction(X, y, k=20, threshold=0.5)\n",
        "        logging.info(f\"Graph construction complete for {dataset_name}. Data object: {data}\")\n",
        "        del X, y\n",
        "        gc.collect()\n",
        "    except MemoryError as e:\n",
        "         logging.error(f\"MemoryError during graph construction for {dataset_name}: {e}. Skipping dataset.\")\n",
        "         if 'X' in locals(): del X\n",
        "         if 'y' in locals(): del y\n",
        "         gc.collect()\n",
        "         continue\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during graph construction for {dataset_name}: {e}\")\n",
        "        if 'X' in locals(): del X\n",
        "        if 'y' in locals(): del y\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    # --- 5. Data Splitting (Masks) ---\n",
        "    logging.info(f\"Creating train/val/test masks for {dataset_name}...\")\n",
        "    try:\n",
        "        num_nodes = data.num_nodes\n",
        "        node_indices = np.arange(num_nodes)\n",
        "        labels_split = data.y.cpu().numpy()\n",
        "\n",
        "        train_ratio = 0.70\n",
        "        val_ratio = 0.15\n",
        "        test_ratio = 0.15\n",
        "\n",
        "        try:\n",
        "            train_indices, temp_indices, _, y_temp = train_test_split(\n",
        "                node_indices, labels_split, train_size=train_ratio, random_state=42, stratify=labels_split\n",
        "            )\n",
        "            val_relative_ratio = val_ratio / (val_ratio + test_ratio)\n",
        "            val_indices, test_indices, _, _ = train_test_split(\n",
        "                temp_indices, y_temp, train_size=val_relative_ratio, random_state=42, stratify=y_temp\n",
        "            )\n",
        "        except ValueError as split_error:\n",
        "             logging.warning(f\"Stratified split failed for {dataset_name}: {split_error}. Using non-stratified split.\")\n",
        "             train_indices, temp_indices = train_test_split(node_indices, train_size=train_ratio, random_state=42)\n",
        "             val_relative_ratio = val_ratio / (val_ratio + test_ratio)\n",
        "             val_indices, test_indices = train_test_split(temp_indices, train_size=val_relative_ratio, random_state=42)\n",
        "\n",
        "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "        train_mask[train_indices] = True\n",
        "        val_mask[val_indices] = True\n",
        "        test_mask[test_indices] = True\n",
        "\n",
        "        data.train_mask = train_mask\n",
        "        data.val_mask = val_mask\n",
        "        data.test_mask = test_mask\n",
        "        logging.info(f\"Masks created for {dataset_name}. Train: {data.train_mask.sum()}, Val: {data.val_mask.sum()}, Test: {data.test_mask.sum()}\")\n",
        "        del node_indices, labels_split, train_indices, temp_indices, val_indices, test_indices\n",
        "        if 'y_temp' in locals(): del y_temp\n",
        "        gc.collect()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during data splitting for {dataset_name}: {e}\")\n",
        "        del data\n",
        "        gc.collect()\n",
        "        continue\n",
        "\n",
        "    # --- 6. Move Data to Device ---\n",
        "    original_device = device # Store the intended device\n",
        "    try:\n",
        "        data = data.to(device)\n",
        "        logging.info(f\"Moved data for {dataset_name} to {device}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to move data for {dataset_name} to {device}: {e}. Trying CPU.\")\n",
        "        try:\n",
        "            device = torch.device('cpu') # Fallback device for this dataset run\n",
        "            data = data.to(device)\n",
        "            logging.info(f\"Using CPU for {dataset_name}.\")\n",
        "        except Exception as cpu_e:\n",
        "             logging.error(f\"Failed to move data for {dataset_name} to CPU: {cpu_e}. Skipping dataset.\")\n",
        "             del data\n",
        "             gc.collect()\n",
        "             continue\n",
        "\n",
        "    # --- 7. Model Training & Evaluation Loop (Inner Loop) ---\n",
        "    node_feat_dim = data.num_node_features\n",
        "    num_classes = len(torch.unique(data.y))\n",
        "    output_dim = 1 if num_classes == 2 else num_classes\n",
        "    is_binary_check = (output_dim == 1)\n",
        "\n",
        "    # Recalculate criterion based on current device/labels\n",
        "    pos_weight_current = None # Reset pos_weight for each dataset\n",
        "    if is_binary_check:\n",
        "        train_labels = data.y[data.train_mask]\n",
        "        num_positives = (train_labels == 1).sum().item()\n",
        "        num_negatives = (train_labels == 0).sum().item()\n",
        "        if num_positives > 0 and num_negatives > 0:\n",
        "            pos_weight_value = num_negatives / num_positives\n",
        "            pos_weight_current = torch.tensor([pos_weight_value], device=device) # Use current device\n",
        "        criterion_cls = nn.BCEWithLogitsLoss(pos_weight=pos_weight_current)\n",
        "    else:\n",
        "        criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    models_to_train = {\n",
        "        \"CAGN\": CAGN(node_feat_dim, hidden_dim, output_dim, heads=gat_heads, dropout=dropout_rate),\n",
        "        \"GCN\": GCN(node_feat_dim, hidden_dim, output_dim),\n",
        "        \"BaselineGAT\": BaselineGAT(node_feat_dim, hidden_dim, output_dim, heads=gat_heads, dropout=dropout_rate),\n",
        "        \"GraphSAGE\": GraphSAGE(node_feat_dim, hidden_dim, output_dim),\n",
        "        \"GIN\": GIN(node_feat_dim, hidden_dim, output_dim)\n",
        "    }\n",
        "\n",
        "    current_dataset_results_list = []\n",
        "    current_dataset_histories = {}\n",
        "\n",
        "    for model_name, model_instance in models_to_train.items():\n",
        "        print(f\"    --- Training {model_name} for {dataset_name} ---\")\n",
        "        logging.info(f\"Starting training for {model_name} on {dataset_name}\")\n",
        "        model = model_instance.to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10, min_lr=1e-6)\n",
        "        is_cagn = isinstance(model, CAGN)\n",
        "        best_val_loss = float('inf')\n",
        "        epochs_no_improve = 0\n",
        "        best_model_state = None\n",
        "        history = {'train_loss': [], 'train_cls_loss': [], 'train_contrast_loss': [],\n",
        "                   'val_loss': [], 'val_cls_loss': [], 'val_contrast_loss': [],\n",
        "                   'val_f1': [], 'val_auc': []}\n",
        "        loop_start_time = time.time()\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            epoch_start_time = time.time()\n",
        "            train_total_loss, train_cls_loss, train_contrast_loss = train_epoch(\n",
        "                model, data, optimizer, criterion_cls, is_cagn_model=is_cagn\n",
        "            )\n",
        "            val_metrics = evaluate(model, data, data.val_mask, criterion_cls, is_cagn_model=is_cagn)\n",
        "            val_loss = val_metrics['loss']\n",
        "            val_f1 = val_metrics['f1']\n",
        "            val_auc = val_metrics['auc']\n",
        "            scheduler_loss = val_metrics['cls_loss'] if not is_cagn else val_loss\n",
        "            scheduler.step(scheduler_loss)\n",
        "\n",
        "            history['train_loss'].append(train_total_loss)\n",
        "            history['train_cls_loss'].append(train_cls_loss)\n",
        "            history['train_contrast_loss'].append(train_contrast_loss)\n",
        "            history['val_loss'].append(val_metrics['loss'])\n",
        "            history['val_cls_loss'].append(val_metrics['cls_loss'])\n",
        "            history['val_contrast_loss'].append(val_metrics['contrast_loss'])\n",
        "            history['val_f1'].append(val_f1)\n",
        "            history['val_auc'].append(val_auc)\n",
        "\n",
        "            epoch_duration = time.time() - epoch_start_time\n",
        "            if epoch % 50 == 0 or epoch == 1:\n",
        "                 train_loss_str = f\"{train_total_loss:.4f} (CLS: {train_cls_loss:.4f}\" + (f\", Contr: {train_contrast_loss:.4f})\" if is_cagn else \")\")\n",
        "                 val_loss_str = f\"{val_metrics['loss']:.4f} (CLS: {val_metrics['cls_loss']:.4f})\"\n",
        "                 print(f'    Epoch: {epoch:03d}, Train Loss: {train_loss_str}, Val Loss: {val_loss_str}, Val F1: {val_f1:.4f}, Val AUC: {val_auc:.4f}')\n",
        "\n",
        "            monitor_loss = val_metrics['cls_loss'] if not is_cagn else val_loss\n",
        "            if monitor_loss < best_val_loss:\n",
        "                best_val_loss = monitor_loss\n",
        "                epochs_no_improve = 0\n",
        "                best_model_state = copy.deepcopy(model.state_dict())\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                logging.info(f\"Early stopping for {model_name} on {dataset_name} at epoch {epoch}.\")\n",
        "                break\n",
        "\n",
        "        total_training_time = time.time() - loop_start_time\n",
        "        current_dataset_histories[model_name] = history\n",
        "\n",
        "        if best_model_state:\n",
        "            model.load_state_dict(best_model_state)\n",
        "        test_metrics = evaluate(model, data, data.test_mask, criterion_cls, is_cagn_model=is_cagn)\n",
        "        print(f\"    Test F1 for {model_name}: {test_metrics['f1']:.4f}, AUC: {test_metrics['auc']:.4f}\")\n",
        "\n",
        "                # --- Save the best model state (Added) ---\n",
        "        # Check if the current model is CAGN and if a best state was found during training\n",
        "        if model_name == \"CAGN\" and best_model_state is not None:\n",
        "            save_dir = \"/media/ssd/test/GNN/Standardized Models/CAGN-GAT/\" # Define save directory\n",
        "            # Save the best model for the specific dataset being processed\n",
        "            save_path = os.path.join(save_dir, f\"best_cagn_model_{dataset_name}.pt\") # Include dataset name in filename\n",
        "            try:\n",
        "                os.makedirs(save_dir, exist_ok=True) # Ensure directory exists\n",
        "                torch.save(best_model_state, save_path) # Save the state dictionary\n",
        "                logging.info(f\"Saved best {model_name} model state for {dataset_name} to {save_path}\")\n",
        "            except Exception as save_e:\n",
        "                logging.error(f\"Error saving {model_name} model state for {dataset_name}: {save_e}\")\n",
        "        # --- End Save Model State ---\n",
        "        \n",
        "        current_dataset_results_list.append({\n",
        "            'Model': model_name,\n",
        "            'Accuracy': test_metrics['accuracy'],\n",
        "            'Precision': test_metrics['precision'],\n",
        "            'Recall': test_metrics['recall'],\n",
        "            'F1': test_metrics['f1'],\n",
        "            'AUC': test_metrics['auc'],\n",
        "            'Test Loss (CLS)': test_metrics['cls_loss'],\n",
        "            'Training Time (s)': total_training_time\n",
        "        })\n",
        "\n",
        "        del model, model_instance, optimizer, scheduler, history, best_model_state, test_metrics\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        # --- End Inner Training Loop Placeholder ---\n",
        "\n",
        "    # --- 8. Store and Display Results for Current Dataset ---\n",
        "    if current_dataset_results_list:\n",
        "        results_df = pd.DataFrame(current_dataset_results_list)\n",
        "        results_df = results_df.sort_values(by='F1', ascending=False)\n",
        "        print(f\"\\n--- Results for {dataset_name} ---\")\n",
        "        print(results_df.round(4).to_markdown(index=False))\n",
        "        all_dataset_results[dataset_name] = results_df\n",
        "        # Optional plotting per dataset can go here\n",
        "\n",
        "    # --- 9. Clean up before next dataset ---\n",
        "    del data\n",
        "    if 'train_mask' in locals(): del train_mask, val_mask, test_mask\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    # Reset device to original intended device if it was changed to CPU\n",
        "    device = original_device\n",
        "\n",
        "\n",
        "# --- End of Dataset Loop ---\n",
        "print(f\"\\n{'='*30}\\nBenchmark Complete\\n{'='*30}\")\n",
        "\n",
        "\n",
        "# --- Final Summary Cell (e.g., New Cell 8) ---\n",
        "print(\"\\n--- Overall Benchmark Summary ---\")\n",
        "for dataset_name, results_df in all_dataset_results.items():\n",
        "    print(f\"\\n--- Results for {dataset_name} ---\")\n",
        "    print(results_df.round(4).to_markdown(index=False))\n",
        "\n",
        "# You could also concatenate results into a single DataFrame\n",
        "# combined_results = pd.concat(all_dataset_results, names=['Dataset']).reset_index(level=0)\n",
        "# print(\"\\n--- Combined Results Table ---\")\n",
        "# print(combined_results.round(4).to_markdown(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 90131,
          "sourceId": 208170,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 111554,
          "sourceId": 267091,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 174616,
          "sourceId": 394223,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3674161,
          "sourceId": 6376134,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30823,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "gnn_cuda_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2171.755824,
      "end_time": "2025-01-31T11:05:53.031601",
      "environment_variables": {},
      "exception": true,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-01-31T10:29:41.275777",
      "version": "2.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
